{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./VILA')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-13 16:29:46,502] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from llava.model import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/m2/holoscan-dev/holoscan-ml/multi-modality-research/VILA/llava/model/llava_arch.py:108: UserWarning: model_dtype not found in config, defaulting to torch.float16.\n",
      "  warnings.warn(\"model_dtype not found in config, defaulting to torch.float16.\")\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:14<00:00,  2.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm.model.embed_tokens.weight torch.Size([32000, 5120])\n",
      "llm.model.layers.0.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.0.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.0.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.0.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.0.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.0.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.0.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "llm.model.layers.0.input_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.0.post_attention_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.1.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.1.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.1.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.1.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.1.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.1.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.1.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "llm.model.layers.1.input_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.1.post_attention_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.2.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.2.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.2.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.2.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.2.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.2.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.2.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "llm.model.layers.2.input_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.2.post_attention_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.3.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.3.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.3.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.3.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.3.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.3.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.3.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "llm.model.layers.3.input_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.3.post_attention_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.4.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.4.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.4.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.4.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.4.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.4.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.4.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "llm.model.layers.4.input_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.4.post_attention_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.5.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.5.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.5.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.5.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.5.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.5.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.5.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "llm.model.layers.5.input_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.5.post_attention_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.6.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.6.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.6.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.6.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.6.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.6.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.6.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "llm.model.layers.6.input_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.6.post_attention_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.7.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.7.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.7.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.7.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.7.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.7.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.7.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "llm.model.layers.7.input_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.7.post_attention_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.8.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.8.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.8.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.8.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.8.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.8.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.8.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "llm.model.layers.8.input_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.8.post_attention_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.9.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.9.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.9.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.9.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.9.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.9.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.9.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "llm.model.layers.9.input_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.9.post_attention_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.10.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.10.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.10.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.10.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.10.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.10.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.10.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "llm.model.layers.10.input_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.10.post_attention_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.11.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.11.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.11.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.11.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.11.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.11.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.11.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "llm.model.layers.11.input_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.11.post_attention_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.12.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.12.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.12.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.12.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.12.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.12.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.12.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "llm.model.layers.12.input_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.12.post_attention_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.13.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.13.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.13.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.13.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.13.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.13.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.13.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "llm.model.layers.13.input_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.13.post_attention_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.14.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.14.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.14.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.14.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.14.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.14.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.14.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "llm.model.layers.14.input_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.14.post_attention_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.15.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.15.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.15.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.15.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.15.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.15.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.15.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "llm.model.layers.15.input_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.15.post_attention_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.16.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.16.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.16.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.16.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.16.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.16.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.16.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "llm.model.layers.16.input_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.16.post_attention_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.17.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.17.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.17.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.17.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.17.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.17.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.17.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "llm.model.layers.17.input_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.17.post_attention_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.18.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.18.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.18.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.18.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.18.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.18.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.18.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "llm.model.layers.18.input_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.18.post_attention_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.19.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.19.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.19.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.19.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.19.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.19.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.19.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "llm.model.layers.19.input_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.19.post_attention_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.20.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.20.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.20.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.20.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.20.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.20.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.20.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "llm.model.layers.20.input_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.20.post_attention_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.21.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.21.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.21.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.21.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.21.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.21.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.21.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "llm.model.layers.21.input_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.21.post_attention_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.22.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.22.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.22.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.22.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.22.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.22.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.22.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "llm.model.layers.22.input_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.22.post_attention_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.23.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.23.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.23.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.23.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.23.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.23.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.23.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "llm.model.layers.23.input_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.23.post_attention_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.24.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.24.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.24.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.24.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.24.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.24.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.24.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "llm.model.layers.24.input_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.24.post_attention_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.25.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.25.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.25.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.25.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.25.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.25.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.25.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "llm.model.layers.25.input_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.25.post_attention_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.26.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.26.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.26.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.26.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.26.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.26.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.26.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "llm.model.layers.26.input_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.26.post_attention_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.27.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.27.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.27.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.27.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.27.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.27.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.27.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "llm.model.layers.27.input_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.27.post_attention_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.28.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.28.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.28.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.28.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.28.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.28.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.28.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "llm.model.layers.28.input_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.28.post_attention_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.29.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.29.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.29.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.29.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.29.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.29.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.29.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "llm.model.layers.29.input_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.29.post_attention_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.30.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.30.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.30.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.30.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.30.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.30.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.30.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "llm.model.layers.30.input_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.30.post_attention_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.31.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.31.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.31.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.31.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.31.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.31.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.31.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "llm.model.layers.31.input_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.31.post_attention_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.32.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.32.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.32.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.32.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.32.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.32.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.32.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "llm.model.layers.32.input_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.32.post_attention_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.33.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.33.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.33.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.33.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.33.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.33.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.33.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "llm.model.layers.33.input_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.33.post_attention_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.34.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.34.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.34.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.34.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.34.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.34.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.34.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "llm.model.layers.34.input_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.34.post_attention_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.35.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.35.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.35.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.35.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.35.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.35.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.35.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "llm.model.layers.35.input_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.35.post_attention_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.36.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.36.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.36.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.36.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.36.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.36.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.36.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "llm.model.layers.36.input_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.36.post_attention_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.37.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.37.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.37.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.37.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.37.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.37.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.37.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "llm.model.layers.37.input_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.37.post_attention_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.38.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.38.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.38.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.38.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.38.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.38.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.38.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "llm.model.layers.38.input_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.38.post_attention_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.39.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.39.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.39.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.39.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "llm.model.layers.39.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.39.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "llm.model.layers.39.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "llm.model.layers.39.input_layernorm.weight torch.Size([5120])\n",
      "llm.model.layers.39.post_attention_layernorm.weight torch.Size([5120])\n",
      "llm.model.norm.weight torch.Size([5120])\n",
      "llm.lm_head.weight torch.Size([32000, 5120])\n",
      "vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight torch.Size([1152, 3, 14, 14])\n",
      "vision_tower.vision_tower.vision_model.embeddings.patch_embedding.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight torch.Size([729, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight torch.Size([4304, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias torch.Size([4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight torch.Size([1152, 4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight torch.Size([4304, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias torch.Size([4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight torch.Size([1152, 4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight torch.Size([4304, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias torch.Size([4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight torch.Size([1152, 4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight torch.Size([4304, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias torch.Size([4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight torch.Size([1152, 4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight torch.Size([4304, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias torch.Size([4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight torch.Size([1152, 4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight torch.Size([4304, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias torch.Size([4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight torch.Size([1152, 4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight torch.Size([4304, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias torch.Size([4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight torch.Size([1152, 4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight torch.Size([4304, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias torch.Size([4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight torch.Size([1152, 4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight torch.Size([4304, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias torch.Size([4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight torch.Size([1152, 4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight torch.Size([4304, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias torch.Size([4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight torch.Size([1152, 4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight torch.Size([4304, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias torch.Size([4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight torch.Size([1152, 4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight torch.Size([4304, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias torch.Size([4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight torch.Size([1152, 4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight torch.Size([4304, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias torch.Size([4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight torch.Size([1152, 4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight torch.Size([4304, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias torch.Size([4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight torch.Size([1152, 4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight torch.Size([4304, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias torch.Size([4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight torch.Size([1152, 4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight torch.Size([4304, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias torch.Size([4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight torch.Size([1152, 4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight torch.Size([4304, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias torch.Size([4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight torch.Size([1152, 4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight torch.Size([4304, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias torch.Size([4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight torch.Size([1152, 4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight torch.Size([4304, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias torch.Size([4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight torch.Size([1152, 4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight torch.Size([4304, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias torch.Size([4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight torch.Size([1152, 4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight torch.Size([4304, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias torch.Size([4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight torch.Size([1152, 4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight torch.Size([4304, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias torch.Size([4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight torch.Size([1152, 4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight torch.Size([4304, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias torch.Size([4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight torch.Size([1152, 4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight torch.Size([4304, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias torch.Size([4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight torch.Size([1152, 4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.24.self_attn.k_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.24.self_attn.v_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.24.self_attn.q_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.24.self_attn.out_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.24.layer_norm1.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.24.layer_norm1.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.24.mlp.fc1.weight torch.Size([4304, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.24.mlp.fc1.bias torch.Size([4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.24.mlp.fc2.weight torch.Size([1152, 4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.24.mlp.fc2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.24.layer_norm2.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.24.layer_norm2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.25.self_attn.k_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.25.self_attn.v_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.25.self_attn.q_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.25.self_attn.out_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.25.layer_norm1.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.25.layer_norm1.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.25.mlp.fc1.weight torch.Size([4304, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.25.mlp.fc1.bias torch.Size([4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.25.mlp.fc2.weight torch.Size([1152, 4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.25.mlp.fc2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.25.layer_norm2.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.25.layer_norm2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.26.self_attn.k_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.26.self_attn.v_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.26.self_attn.q_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.26.self_attn.out_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.26.layer_norm1.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.26.layer_norm1.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.26.mlp.fc1.weight torch.Size([4304, 1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.26.mlp.fc1.bias torch.Size([4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.26.mlp.fc2.weight torch.Size([1152, 4304])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.26.mlp.fc2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.26.layer_norm2.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.encoder.layers.26.layer_norm2.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.post_layernorm.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.post_layernorm.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.head.probe torch.Size([1, 1, 1152])\n",
      "vision_tower.vision_tower.vision_model.head.attention.in_proj_weight torch.Size([3456, 1152])\n",
      "vision_tower.vision_tower.vision_model.head.attention.in_proj_bias torch.Size([3456])\n",
      "vision_tower.vision_tower.vision_model.head.attention.out_proj.weight torch.Size([1152, 1152])\n",
      "vision_tower.vision_tower.vision_model.head.attention.out_proj.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.head.layernorm.weight torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.head.layernorm.bias torch.Size([1152])\n",
      "vision_tower.vision_tower.vision_model.head.mlp.fc1.weight torch.Size([4304, 1152])\n",
      "vision_tower.vision_tower.vision_model.head.mlp.fc1.bias torch.Size([4304])\n",
      "vision_tower.vision_tower.vision_model.head.mlp.fc2.weight torch.Size([1152, 4304])\n",
      "vision_tower.vision_tower.vision_model.head.mlp.fc2.bias torch.Size([1152])\n",
      "mm_projector.layers.1.weight torch.Size([4608])\n",
      "mm_projector.layers.1.bias torch.Size([4608])\n",
      "mm_projector.layers.2.weight torch.Size([5120, 4608])\n",
      "mm_projector.layers.2.bias torch.Size([5120])\n",
      "mm_projector.layers.4.weight torch.Size([5120, 5120])\n",
      "mm_projector.layers.4.bias torch.Size([5120])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model = \"/media/m3/models/CholecT50_vila_13B_3_Epoch\"\n",
    "# find the model part that includes the the multimodal projector weights\n",
    "model = AutoModel.from_pretrained(model, trust_remote_code=True, local_files_only=True)\n",
    "checkpoint = model.state_dict()\n",
    "\n",
    "for k, v in checkpoint.items():\n",
    "    print(k, v.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/m3/conda/vila.cpp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:18<00:00,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm.model.embed_tokens.weight\n",
      "llm.model.layers.0.self_attn.q_proj.weight\n",
      "llm.model.layers.0.self_attn.q_proj.bias\n",
      "llm.model.layers.0.self_attn.k_proj.weight\n",
      "llm.model.layers.0.self_attn.k_proj.bias\n",
      "llm.model.layers.0.self_attn.v_proj.weight\n",
      "llm.model.layers.0.self_attn.v_proj.bias\n",
      "llm.model.layers.0.self_attn.o_proj.weight\n",
      "llm.model.layers.0.mlp.gate_proj.weight\n",
      "llm.model.layers.0.mlp.up_proj.weight\n",
      "llm.model.layers.0.mlp.down_proj.weight\n",
      "llm.model.layers.0.input_layernorm.weight\n",
      "llm.model.layers.0.post_attention_layernorm.weight\n",
      "llm.model.layers.1.self_attn.q_proj.weight\n",
      "llm.model.layers.1.self_attn.q_proj.bias\n",
      "llm.model.layers.1.self_attn.k_proj.weight\n",
      "llm.model.layers.1.self_attn.k_proj.bias\n",
      "llm.model.layers.1.self_attn.v_proj.weight\n",
      "llm.model.layers.1.self_attn.v_proj.bias\n",
      "llm.model.layers.1.self_attn.o_proj.weight\n",
      "llm.model.layers.1.mlp.gate_proj.weight\n",
      "llm.model.layers.1.mlp.up_proj.weight\n",
      "llm.model.layers.1.mlp.down_proj.weight\n",
      "llm.model.layers.1.input_layernorm.weight\n",
      "llm.model.layers.1.post_attention_layernorm.weight\n",
      "llm.model.layers.2.self_attn.q_proj.weight\n",
      "llm.model.layers.2.self_attn.q_proj.bias\n",
      "llm.model.layers.2.self_attn.k_proj.weight\n",
      "llm.model.layers.2.self_attn.k_proj.bias\n",
      "llm.model.layers.2.self_attn.v_proj.weight\n",
      "llm.model.layers.2.self_attn.v_proj.bias\n",
      "llm.model.layers.2.self_attn.o_proj.weight\n",
      "llm.model.layers.2.mlp.gate_proj.weight\n",
      "llm.model.layers.2.mlp.up_proj.weight\n",
      "llm.model.layers.2.mlp.down_proj.weight\n",
      "llm.model.layers.2.input_layernorm.weight\n",
      "llm.model.layers.2.post_attention_layernorm.weight\n",
      "llm.model.layers.3.self_attn.q_proj.weight\n",
      "llm.model.layers.3.self_attn.q_proj.bias\n",
      "llm.model.layers.3.self_attn.k_proj.weight\n",
      "llm.model.layers.3.self_attn.k_proj.bias\n",
      "llm.model.layers.3.self_attn.v_proj.weight\n",
      "llm.model.layers.3.self_attn.v_proj.bias\n",
      "llm.model.layers.3.self_attn.o_proj.weight\n",
      "llm.model.layers.3.mlp.gate_proj.weight\n",
      "llm.model.layers.3.mlp.up_proj.weight\n",
      "llm.model.layers.3.mlp.down_proj.weight\n",
      "llm.model.layers.3.input_layernorm.weight\n",
      "llm.model.layers.3.post_attention_layernorm.weight\n",
      "llm.model.layers.4.self_attn.q_proj.weight\n",
      "llm.model.layers.4.self_attn.q_proj.bias\n",
      "llm.model.layers.4.self_attn.k_proj.weight\n",
      "llm.model.layers.4.self_attn.k_proj.bias\n",
      "llm.model.layers.4.self_attn.v_proj.weight\n",
      "llm.model.layers.4.self_attn.v_proj.bias\n",
      "llm.model.layers.4.self_attn.o_proj.weight\n",
      "llm.model.layers.4.mlp.gate_proj.weight\n",
      "llm.model.layers.4.mlp.up_proj.weight\n",
      "llm.model.layers.4.mlp.down_proj.weight\n",
      "llm.model.layers.4.input_layernorm.weight\n",
      "llm.model.layers.4.post_attention_layernorm.weight\n",
      "llm.model.layers.5.self_attn.q_proj.weight\n",
      "llm.model.layers.5.self_attn.q_proj.bias\n",
      "llm.model.layers.5.self_attn.k_proj.weight\n",
      "llm.model.layers.5.self_attn.k_proj.bias\n",
      "llm.model.layers.5.self_attn.v_proj.weight\n",
      "llm.model.layers.5.self_attn.v_proj.bias\n",
      "llm.model.layers.5.self_attn.o_proj.weight\n",
      "llm.model.layers.5.mlp.gate_proj.weight\n",
      "llm.model.layers.5.mlp.up_proj.weight\n",
      "llm.model.layers.5.mlp.down_proj.weight\n",
      "llm.model.layers.5.input_layernorm.weight\n",
      "llm.model.layers.5.post_attention_layernorm.weight\n",
      "llm.model.layers.6.self_attn.q_proj.weight\n",
      "llm.model.layers.6.self_attn.q_proj.bias\n",
      "llm.model.layers.6.self_attn.k_proj.weight\n",
      "llm.model.layers.6.self_attn.k_proj.bias\n",
      "llm.model.layers.6.self_attn.v_proj.weight\n",
      "llm.model.layers.6.self_attn.v_proj.bias\n",
      "llm.model.layers.6.self_attn.o_proj.weight\n",
      "llm.model.layers.6.mlp.gate_proj.weight\n",
      "llm.model.layers.6.mlp.up_proj.weight\n",
      "llm.model.layers.6.mlp.down_proj.weight\n",
      "llm.model.layers.6.input_layernorm.weight\n",
      "llm.model.layers.6.post_attention_layernorm.weight\n",
      "llm.model.layers.7.self_attn.q_proj.weight\n",
      "llm.model.layers.7.self_attn.q_proj.bias\n",
      "llm.model.layers.7.self_attn.k_proj.weight\n",
      "llm.model.layers.7.self_attn.k_proj.bias\n",
      "llm.model.layers.7.self_attn.v_proj.weight\n",
      "llm.model.layers.7.self_attn.v_proj.bias\n",
      "llm.model.layers.7.self_attn.o_proj.weight\n",
      "llm.model.layers.7.mlp.gate_proj.weight\n",
      "llm.model.layers.7.mlp.up_proj.weight\n",
      "llm.model.layers.7.mlp.down_proj.weight\n",
      "llm.model.layers.7.input_layernorm.weight\n",
      "llm.model.layers.7.post_attention_layernorm.weight\n",
      "llm.model.layers.8.self_attn.q_proj.weight\n",
      "llm.model.layers.8.self_attn.q_proj.bias\n",
      "llm.model.layers.8.self_attn.k_proj.weight\n",
      "llm.model.layers.8.self_attn.k_proj.bias\n",
      "llm.model.layers.8.self_attn.v_proj.weight\n",
      "llm.model.layers.8.self_attn.v_proj.bias\n",
      "llm.model.layers.8.self_attn.o_proj.weight\n",
      "llm.model.layers.8.mlp.gate_proj.weight\n",
      "llm.model.layers.8.mlp.up_proj.weight\n",
      "llm.model.layers.8.mlp.down_proj.weight\n",
      "llm.model.layers.8.input_layernorm.weight\n",
      "llm.model.layers.8.post_attention_layernorm.weight\n",
      "llm.model.layers.9.self_attn.q_proj.weight\n",
      "llm.model.layers.9.self_attn.q_proj.bias\n",
      "llm.model.layers.9.self_attn.k_proj.weight\n",
      "llm.model.layers.9.self_attn.k_proj.bias\n",
      "llm.model.layers.9.self_attn.v_proj.weight\n",
      "llm.model.layers.9.self_attn.v_proj.bias\n",
      "llm.model.layers.9.self_attn.o_proj.weight\n",
      "llm.model.layers.9.mlp.gate_proj.weight\n",
      "llm.model.layers.9.mlp.up_proj.weight\n",
      "llm.model.layers.9.mlp.down_proj.weight\n",
      "llm.model.layers.9.input_layernorm.weight\n",
      "llm.model.layers.9.post_attention_layernorm.weight\n",
      "llm.model.layers.10.self_attn.q_proj.weight\n",
      "llm.model.layers.10.self_attn.q_proj.bias\n",
      "llm.model.layers.10.self_attn.k_proj.weight\n",
      "llm.model.layers.10.self_attn.k_proj.bias\n",
      "llm.model.layers.10.self_attn.v_proj.weight\n",
      "llm.model.layers.10.self_attn.v_proj.bias\n",
      "llm.model.layers.10.self_attn.o_proj.weight\n",
      "llm.model.layers.10.mlp.gate_proj.weight\n",
      "llm.model.layers.10.mlp.up_proj.weight\n",
      "llm.model.layers.10.mlp.down_proj.weight\n",
      "llm.model.layers.10.input_layernorm.weight\n",
      "llm.model.layers.10.post_attention_layernorm.weight\n",
      "llm.model.layers.11.self_attn.q_proj.weight\n",
      "llm.model.layers.11.self_attn.q_proj.bias\n",
      "llm.model.layers.11.self_attn.k_proj.weight\n",
      "llm.model.layers.11.self_attn.k_proj.bias\n",
      "llm.model.layers.11.self_attn.v_proj.weight\n",
      "llm.model.layers.11.self_attn.v_proj.bias\n",
      "llm.model.layers.11.self_attn.o_proj.weight\n",
      "llm.model.layers.11.mlp.gate_proj.weight\n",
      "llm.model.layers.11.mlp.up_proj.weight\n",
      "llm.model.layers.11.mlp.down_proj.weight\n",
      "llm.model.layers.11.input_layernorm.weight\n",
      "llm.model.layers.11.post_attention_layernorm.weight\n",
      "llm.model.layers.12.self_attn.q_proj.weight\n",
      "llm.model.layers.12.self_attn.q_proj.bias\n",
      "llm.model.layers.12.self_attn.k_proj.weight\n",
      "llm.model.layers.12.self_attn.k_proj.bias\n",
      "llm.model.layers.12.self_attn.v_proj.weight\n",
      "llm.model.layers.12.self_attn.v_proj.bias\n",
      "llm.model.layers.12.self_attn.o_proj.weight\n",
      "llm.model.layers.12.mlp.gate_proj.weight\n",
      "llm.model.layers.12.mlp.up_proj.weight\n",
      "llm.model.layers.12.mlp.down_proj.weight\n",
      "llm.model.layers.12.input_layernorm.weight\n",
      "llm.model.layers.12.post_attention_layernorm.weight\n",
      "llm.model.layers.13.self_attn.q_proj.weight\n",
      "llm.model.layers.13.self_attn.q_proj.bias\n",
      "llm.model.layers.13.self_attn.k_proj.weight\n",
      "llm.model.layers.13.self_attn.k_proj.bias\n",
      "llm.model.layers.13.self_attn.v_proj.weight\n",
      "llm.model.layers.13.self_attn.v_proj.bias\n",
      "llm.model.layers.13.self_attn.o_proj.weight\n",
      "llm.model.layers.13.mlp.gate_proj.weight\n",
      "llm.model.layers.13.mlp.up_proj.weight\n",
      "llm.model.layers.13.mlp.down_proj.weight\n",
      "llm.model.layers.13.input_layernorm.weight\n",
      "llm.model.layers.13.post_attention_layernorm.weight\n",
      "llm.model.layers.14.self_attn.q_proj.weight\n",
      "llm.model.layers.14.self_attn.q_proj.bias\n",
      "llm.model.layers.14.self_attn.k_proj.weight\n",
      "llm.model.layers.14.self_attn.k_proj.bias\n",
      "llm.model.layers.14.self_attn.v_proj.weight\n",
      "llm.model.layers.14.self_attn.v_proj.bias\n",
      "llm.model.layers.14.self_attn.o_proj.weight\n",
      "llm.model.layers.14.mlp.gate_proj.weight\n",
      "llm.model.layers.14.mlp.up_proj.weight\n",
      "llm.model.layers.14.mlp.down_proj.weight\n",
      "llm.model.layers.14.input_layernorm.weight\n",
      "llm.model.layers.14.post_attention_layernorm.weight\n",
      "llm.model.layers.15.self_attn.q_proj.weight\n",
      "llm.model.layers.15.self_attn.q_proj.bias\n",
      "llm.model.layers.15.self_attn.k_proj.weight\n",
      "llm.model.layers.15.self_attn.k_proj.bias\n",
      "llm.model.layers.15.self_attn.v_proj.weight\n",
      "llm.model.layers.15.self_attn.v_proj.bias\n",
      "llm.model.layers.15.self_attn.o_proj.weight\n",
      "llm.model.layers.15.mlp.gate_proj.weight\n",
      "llm.model.layers.15.mlp.up_proj.weight\n",
      "llm.model.layers.15.mlp.down_proj.weight\n",
      "llm.model.layers.15.input_layernorm.weight\n",
      "llm.model.layers.15.post_attention_layernorm.weight\n",
      "llm.model.layers.16.self_attn.q_proj.weight\n",
      "llm.model.layers.16.self_attn.q_proj.bias\n",
      "llm.model.layers.16.self_attn.k_proj.weight\n",
      "llm.model.layers.16.self_attn.k_proj.bias\n",
      "llm.model.layers.16.self_attn.v_proj.weight\n",
      "llm.model.layers.16.self_attn.v_proj.bias\n",
      "llm.model.layers.16.self_attn.o_proj.weight\n",
      "llm.model.layers.16.mlp.gate_proj.weight\n",
      "llm.model.layers.16.mlp.up_proj.weight\n",
      "llm.model.layers.16.mlp.down_proj.weight\n",
      "llm.model.layers.16.input_layernorm.weight\n",
      "llm.model.layers.16.post_attention_layernorm.weight\n",
      "llm.model.layers.17.self_attn.q_proj.weight\n",
      "llm.model.layers.17.self_attn.q_proj.bias\n",
      "llm.model.layers.17.self_attn.k_proj.weight\n",
      "llm.model.layers.17.self_attn.k_proj.bias\n",
      "llm.model.layers.17.self_attn.v_proj.weight\n",
      "llm.model.layers.17.self_attn.v_proj.bias\n",
      "llm.model.layers.17.self_attn.o_proj.weight\n",
      "llm.model.layers.17.mlp.gate_proj.weight\n",
      "llm.model.layers.17.mlp.up_proj.weight\n",
      "llm.model.layers.17.mlp.down_proj.weight\n",
      "llm.model.layers.17.input_layernorm.weight\n",
      "llm.model.layers.17.post_attention_layernorm.weight\n",
      "llm.model.layers.18.self_attn.q_proj.weight\n",
      "llm.model.layers.18.self_attn.q_proj.bias\n",
      "llm.model.layers.18.self_attn.k_proj.weight\n",
      "llm.model.layers.18.self_attn.k_proj.bias\n",
      "llm.model.layers.18.self_attn.v_proj.weight\n",
      "llm.model.layers.18.self_attn.v_proj.bias\n",
      "llm.model.layers.18.self_attn.o_proj.weight\n",
      "llm.model.layers.18.mlp.gate_proj.weight\n",
      "llm.model.layers.18.mlp.up_proj.weight\n",
      "llm.model.layers.18.mlp.down_proj.weight\n",
      "llm.model.layers.18.input_layernorm.weight\n",
      "llm.model.layers.18.post_attention_layernorm.weight\n",
      "llm.model.layers.19.self_attn.q_proj.weight\n",
      "llm.model.layers.19.self_attn.q_proj.bias\n",
      "llm.model.layers.19.self_attn.k_proj.weight\n",
      "llm.model.layers.19.self_attn.k_proj.bias\n",
      "llm.model.layers.19.self_attn.v_proj.weight\n",
      "llm.model.layers.19.self_attn.v_proj.bias\n",
      "llm.model.layers.19.self_attn.o_proj.weight\n",
      "llm.model.layers.19.mlp.gate_proj.weight\n",
      "llm.model.layers.19.mlp.up_proj.weight\n",
      "llm.model.layers.19.mlp.down_proj.weight\n",
      "llm.model.layers.19.input_layernorm.weight\n",
      "llm.model.layers.19.post_attention_layernorm.weight\n",
      "llm.model.layers.20.self_attn.q_proj.weight\n",
      "llm.model.layers.20.self_attn.q_proj.bias\n",
      "llm.model.layers.20.self_attn.k_proj.weight\n",
      "llm.model.layers.20.self_attn.k_proj.bias\n",
      "llm.model.layers.20.self_attn.v_proj.weight\n",
      "llm.model.layers.20.self_attn.v_proj.bias\n",
      "llm.model.layers.20.self_attn.o_proj.weight\n",
      "llm.model.layers.20.mlp.gate_proj.weight\n",
      "llm.model.layers.20.mlp.up_proj.weight\n",
      "llm.model.layers.20.mlp.down_proj.weight\n",
      "llm.model.layers.20.input_layernorm.weight\n",
      "llm.model.layers.20.post_attention_layernorm.weight\n",
      "llm.model.layers.21.self_attn.q_proj.weight\n",
      "llm.model.layers.21.self_attn.q_proj.bias\n",
      "llm.model.layers.21.self_attn.k_proj.weight\n",
      "llm.model.layers.21.self_attn.k_proj.bias\n",
      "llm.model.layers.21.self_attn.v_proj.weight\n",
      "llm.model.layers.21.self_attn.v_proj.bias\n",
      "llm.model.layers.21.self_attn.o_proj.weight\n",
      "llm.model.layers.21.mlp.gate_proj.weight\n",
      "llm.model.layers.21.mlp.up_proj.weight\n",
      "llm.model.layers.21.mlp.down_proj.weight\n",
      "llm.model.layers.21.input_layernorm.weight\n",
      "llm.model.layers.21.post_attention_layernorm.weight\n",
      "llm.model.layers.22.self_attn.q_proj.weight\n",
      "llm.model.layers.22.self_attn.q_proj.bias\n",
      "llm.model.layers.22.self_attn.k_proj.weight\n",
      "llm.model.layers.22.self_attn.k_proj.bias\n",
      "llm.model.layers.22.self_attn.v_proj.weight\n",
      "llm.model.layers.22.self_attn.v_proj.bias\n",
      "llm.model.layers.22.self_attn.o_proj.weight\n",
      "llm.model.layers.22.mlp.gate_proj.weight\n",
      "llm.model.layers.22.mlp.up_proj.weight\n",
      "llm.model.layers.22.mlp.down_proj.weight\n",
      "llm.model.layers.22.input_layernorm.weight\n",
      "llm.model.layers.22.post_attention_layernorm.weight\n",
      "llm.model.layers.23.self_attn.q_proj.weight\n",
      "llm.model.layers.23.self_attn.q_proj.bias\n",
      "llm.model.layers.23.self_attn.k_proj.weight\n",
      "llm.model.layers.23.self_attn.k_proj.bias\n",
      "llm.model.layers.23.self_attn.v_proj.weight\n",
      "llm.model.layers.23.self_attn.v_proj.bias\n",
      "llm.model.layers.23.self_attn.o_proj.weight\n",
      "llm.model.layers.23.mlp.gate_proj.weight\n",
      "llm.model.layers.23.mlp.up_proj.weight\n",
      "llm.model.layers.23.mlp.down_proj.weight\n",
      "llm.model.layers.23.input_layernorm.weight\n",
      "llm.model.layers.23.post_attention_layernorm.weight\n",
      "llm.model.layers.24.self_attn.q_proj.weight\n",
      "llm.model.layers.24.self_attn.q_proj.bias\n",
      "llm.model.layers.24.self_attn.k_proj.weight\n",
      "llm.model.layers.24.self_attn.k_proj.bias\n",
      "llm.model.layers.24.self_attn.v_proj.weight\n",
      "llm.model.layers.24.self_attn.v_proj.bias\n",
      "llm.model.layers.24.self_attn.o_proj.weight\n",
      "llm.model.layers.24.mlp.gate_proj.weight\n",
      "llm.model.layers.24.mlp.up_proj.weight\n",
      "llm.model.layers.24.mlp.down_proj.weight\n",
      "llm.model.layers.24.input_layernorm.weight\n",
      "llm.model.layers.24.post_attention_layernorm.weight\n",
      "llm.model.layers.25.self_attn.q_proj.weight\n",
      "llm.model.layers.25.self_attn.q_proj.bias\n",
      "llm.model.layers.25.self_attn.k_proj.weight\n",
      "llm.model.layers.25.self_attn.k_proj.bias\n",
      "llm.model.layers.25.self_attn.v_proj.weight\n",
      "llm.model.layers.25.self_attn.v_proj.bias\n",
      "llm.model.layers.25.self_attn.o_proj.weight\n",
      "llm.model.layers.25.mlp.gate_proj.weight\n",
      "llm.model.layers.25.mlp.up_proj.weight\n",
      "llm.model.layers.25.mlp.down_proj.weight\n",
      "llm.model.layers.25.input_layernorm.weight\n",
      "llm.model.layers.25.post_attention_layernorm.weight\n",
      "llm.model.layers.26.self_attn.q_proj.weight\n",
      "llm.model.layers.26.self_attn.q_proj.bias\n",
      "llm.model.layers.26.self_attn.k_proj.weight\n",
      "llm.model.layers.26.self_attn.k_proj.bias\n",
      "llm.model.layers.26.self_attn.v_proj.weight\n",
      "llm.model.layers.26.self_attn.v_proj.bias\n",
      "llm.model.layers.26.self_attn.o_proj.weight\n",
      "llm.model.layers.26.mlp.gate_proj.weight\n",
      "llm.model.layers.26.mlp.up_proj.weight\n",
      "llm.model.layers.26.mlp.down_proj.weight\n",
      "llm.model.layers.26.input_layernorm.weight\n",
      "llm.model.layers.26.post_attention_layernorm.weight\n",
      "llm.model.layers.27.self_attn.q_proj.weight\n",
      "llm.model.layers.27.self_attn.q_proj.bias\n",
      "llm.model.layers.27.self_attn.k_proj.weight\n",
      "llm.model.layers.27.self_attn.k_proj.bias\n",
      "llm.model.layers.27.self_attn.v_proj.weight\n",
      "llm.model.layers.27.self_attn.v_proj.bias\n",
      "llm.model.layers.27.self_attn.o_proj.weight\n",
      "llm.model.layers.27.mlp.gate_proj.weight\n",
      "llm.model.layers.27.mlp.up_proj.weight\n",
      "llm.model.layers.27.mlp.down_proj.weight\n",
      "llm.model.layers.27.input_layernorm.weight\n",
      "llm.model.layers.27.post_attention_layernorm.weight\n",
      "llm.model.norm.weight\n",
      "llm.lm_head.weight\n",
      "vpm.embeddings.patch_embedding.weight\n",
      "vpm.embeddings.patch_embedding.bias\n",
      "vpm.embeddings.position_embedding.weight\n",
      "vpm.encoder.layers.0.self_attn.k_proj.weight\n",
      "vpm.encoder.layers.0.self_attn.k_proj.bias\n",
      "vpm.encoder.layers.0.self_attn.v_proj.weight\n",
      "vpm.encoder.layers.0.self_attn.v_proj.bias\n",
      "vpm.encoder.layers.0.self_attn.q_proj.weight\n",
      "vpm.encoder.layers.0.self_attn.q_proj.bias\n",
      "vpm.encoder.layers.0.self_attn.out_proj.weight\n",
      "vpm.encoder.layers.0.self_attn.out_proj.bias\n",
      "vpm.encoder.layers.0.layer_norm1.weight\n",
      "vpm.encoder.layers.0.layer_norm1.bias\n",
      "vpm.encoder.layers.0.mlp.fc1.weight\n",
      "vpm.encoder.layers.0.mlp.fc1.bias\n",
      "vpm.encoder.layers.0.mlp.fc2.weight\n",
      "vpm.encoder.layers.0.mlp.fc2.bias\n",
      "vpm.encoder.layers.0.layer_norm2.weight\n",
      "vpm.encoder.layers.0.layer_norm2.bias\n",
      "vpm.encoder.layers.1.self_attn.k_proj.weight\n",
      "vpm.encoder.layers.1.self_attn.k_proj.bias\n",
      "vpm.encoder.layers.1.self_attn.v_proj.weight\n",
      "vpm.encoder.layers.1.self_attn.v_proj.bias\n",
      "vpm.encoder.layers.1.self_attn.q_proj.weight\n",
      "vpm.encoder.layers.1.self_attn.q_proj.bias\n",
      "vpm.encoder.layers.1.self_attn.out_proj.weight\n",
      "vpm.encoder.layers.1.self_attn.out_proj.bias\n",
      "vpm.encoder.layers.1.layer_norm1.weight\n",
      "vpm.encoder.layers.1.layer_norm1.bias\n",
      "vpm.encoder.layers.1.mlp.fc1.weight\n",
      "vpm.encoder.layers.1.mlp.fc1.bias\n",
      "vpm.encoder.layers.1.mlp.fc2.weight\n",
      "vpm.encoder.layers.1.mlp.fc2.bias\n",
      "vpm.encoder.layers.1.layer_norm2.weight\n",
      "vpm.encoder.layers.1.layer_norm2.bias\n",
      "vpm.encoder.layers.2.self_attn.k_proj.weight\n",
      "vpm.encoder.layers.2.self_attn.k_proj.bias\n",
      "vpm.encoder.layers.2.self_attn.v_proj.weight\n",
      "vpm.encoder.layers.2.self_attn.v_proj.bias\n",
      "vpm.encoder.layers.2.self_attn.q_proj.weight\n",
      "vpm.encoder.layers.2.self_attn.q_proj.bias\n",
      "vpm.encoder.layers.2.self_attn.out_proj.weight\n",
      "vpm.encoder.layers.2.self_attn.out_proj.bias\n",
      "vpm.encoder.layers.2.layer_norm1.weight\n",
      "vpm.encoder.layers.2.layer_norm1.bias\n",
      "vpm.encoder.layers.2.mlp.fc1.weight\n",
      "vpm.encoder.layers.2.mlp.fc1.bias\n",
      "vpm.encoder.layers.2.mlp.fc2.weight\n",
      "vpm.encoder.layers.2.mlp.fc2.bias\n",
      "vpm.encoder.layers.2.layer_norm2.weight\n",
      "vpm.encoder.layers.2.layer_norm2.bias\n",
      "vpm.encoder.layers.3.self_attn.k_proj.weight\n",
      "vpm.encoder.layers.3.self_attn.k_proj.bias\n",
      "vpm.encoder.layers.3.self_attn.v_proj.weight\n",
      "vpm.encoder.layers.3.self_attn.v_proj.bias\n",
      "vpm.encoder.layers.3.self_attn.q_proj.weight\n",
      "vpm.encoder.layers.3.self_attn.q_proj.bias\n",
      "vpm.encoder.layers.3.self_attn.out_proj.weight\n",
      "vpm.encoder.layers.3.self_attn.out_proj.bias\n",
      "vpm.encoder.layers.3.layer_norm1.weight\n",
      "vpm.encoder.layers.3.layer_norm1.bias\n",
      "vpm.encoder.layers.3.mlp.fc1.weight\n",
      "vpm.encoder.layers.3.mlp.fc1.bias\n",
      "vpm.encoder.layers.3.mlp.fc2.weight\n",
      "vpm.encoder.layers.3.mlp.fc2.bias\n",
      "vpm.encoder.layers.3.layer_norm2.weight\n",
      "vpm.encoder.layers.3.layer_norm2.bias\n",
      "vpm.encoder.layers.4.self_attn.k_proj.weight\n",
      "vpm.encoder.layers.4.self_attn.k_proj.bias\n",
      "vpm.encoder.layers.4.self_attn.v_proj.weight\n",
      "vpm.encoder.layers.4.self_attn.v_proj.bias\n",
      "vpm.encoder.layers.4.self_attn.q_proj.weight\n",
      "vpm.encoder.layers.4.self_attn.q_proj.bias\n",
      "vpm.encoder.layers.4.self_attn.out_proj.weight\n",
      "vpm.encoder.layers.4.self_attn.out_proj.bias\n",
      "vpm.encoder.layers.4.layer_norm1.weight\n",
      "vpm.encoder.layers.4.layer_norm1.bias\n",
      "vpm.encoder.layers.4.mlp.fc1.weight\n",
      "vpm.encoder.layers.4.mlp.fc1.bias\n",
      "vpm.encoder.layers.4.mlp.fc2.weight\n",
      "vpm.encoder.layers.4.mlp.fc2.bias\n",
      "vpm.encoder.layers.4.layer_norm2.weight\n",
      "vpm.encoder.layers.4.layer_norm2.bias\n",
      "vpm.encoder.layers.5.self_attn.k_proj.weight\n",
      "vpm.encoder.layers.5.self_attn.k_proj.bias\n",
      "vpm.encoder.layers.5.self_attn.v_proj.weight\n",
      "vpm.encoder.layers.5.self_attn.v_proj.bias\n",
      "vpm.encoder.layers.5.self_attn.q_proj.weight\n",
      "vpm.encoder.layers.5.self_attn.q_proj.bias\n",
      "vpm.encoder.layers.5.self_attn.out_proj.weight\n",
      "vpm.encoder.layers.5.self_attn.out_proj.bias\n",
      "vpm.encoder.layers.5.layer_norm1.weight\n",
      "vpm.encoder.layers.5.layer_norm1.bias\n",
      "vpm.encoder.layers.5.mlp.fc1.weight\n",
      "vpm.encoder.layers.5.mlp.fc1.bias\n",
      "vpm.encoder.layers.5.mlp.fc2.weight\n",
      "vpm.encoder.layers.5.mlp.fc2.bias\n",
      "vpm.encoder.layers.5.layer_norm2.weight\n",
      "vpm.encoder.layers.5.layer_norm2.bias\n",
      "vpm.encoder.layers.6.self_attn.k_proj.weight\n",
      "vpm.encoder.layers.6.self_attn.k_proj.bias\n",
      "vpm.encoder.layers.6.self_attn.v_proj.weight\n",
      "vpm.encoder.layers.6.self_attn.v_proj.bias\n",
      "vpm.encoder.layers.6.self_attn.q_proj.weight\n",
      "vpm.encoder.layers.6.self_attn.q_proj.bias\n",
      "vpm.encoder.layers.6.self_attn.out_proj.weight\n",
      "vpm.encoder.layers.6.self_attn.out_proj.bias\n",
      "vpm.encoder.layers.6.layer_norm1.weight\n",
      "vpm.encoder.layers.6.layer_norm1.bias\n",
      "vpm.encoder.layers.6.mlp.fc1.weight\n",
      "vpm.encoder.layers.6.mlp.fc1.bias\n",
      "vpm.encoder.layers.6.mlp.fc2.weight\n",
      "vpm.encoder.layers.6.mlp.fc2.bias\n",
      "vpm.encoder.layers.6.layer_norm2.weight\n",
      "vpm.encoder.layers.6.layer_norm2.bias\n",
      "vpm.encoder.layers.7.self_attn.k_proj.weight\n",
      "vpm.encoder.layers.7.self_attn.k_proj.bias\n",
      "vpm.encoder.layers.7.self_attn.v_proj.weight\n",
      "vpm.encoder.layers.7.self_attn.v_proj.bias\n",
      "vpm.encoder.layers.7.self_attn.q_proj.weight\n",
      "vpm.encoder.layers.7.self_attn.q_proj.bias\n",
      "vpm.encoder.layers.7.self_attn.out_proj.weight\n",
      "vpm.encoder.layers.7.self_attn.out_proj.bias\n",
      "vpm.encoder.layers.7.layer_norm1.weight\n",
      "vpm.encoder.layers.7.layer_norm1.bias\n",
      "vpm.encoder.layers.7.mlp.fc1.weight\n",
      "vpm.encoder.layers.7.mlp.fc1.bias\n",
      "vpm.encoder.layers.7.mlp.fc2.weight\n",
      "vpm.encoder.layers.7.mlp.fc2.bias\n",
      "vpm.encoder.layers.7.layer_norm2.weight\n",
      "vpm.encoder.layers.7.layer_norm2.bias\n",
      "vpm.encoder.layers.8.self_attn.k_proj.weight\n",
      "vpm.encoder.layers.8.self_attn.k_proj.bias\n",
      "vpm.encoder.layers.8.self_attn.v_proj.weight\n",
      "vpm.encoder.layers.8.self_attn.v_proj.bias\n",
      "vpm.encoder.layers.8.self_attn.q_proj.weight\n",
      "vpm.encoder.layers.8.self_attn.q_proj.bias\n",
      "vpm.encoder.layers.8.self_attn.out_proj.weight\n",
      "vpm.encoder.layers.8.self_attn.out_proj.bias\n",
      "vpm.encoder.layers.8.layer_norm1.weight\n",
      "vpm.encoder.layers.8.layer_norm1.bias\n",
      "vpm.encoder.layers.8.mlp.fc1.weight\n",
      "vpm.encoder.layers.8.mlp.fc1.bias\n",
      "vpm.encoder.layers.8.mlp.fc2.weight\n",
      "vpm.encoder.layers.8.mlp.fc2.bias\n",
      "vpm.encoder.layers.8.layer_norm2.weight\n",
      "vpm.encoder.layers.8.layer_norm2.bias\n",
      "vpm.encoder.layers.9.self_attn.k_proj.weight\n",
      "vpm.encoder.layers.9.self_attn.k_proj.bias\n",
      "vpm.encoder.layers.9.self_attn.v_proj.weight\n",
      "vpm.encoder.layers.9.self_attn.v_proj.bias\n",
      "vpm.encoder.layers.9.self_attn.q_proj.weight\n",
      "vpm.encoder.layers.9.self_attn.q_proj.bias\n",
      "vpm.encoder.layers.9.self_attn.out_proj.weight\n",
      "vpm.encoder.layers.9.self_attn.out_proj.bias\n",
      "vpm.encoder.layers.9.layer_norm1.weight\n",
      "vpm.encoder.layers.9.layer_norm1.bias\n",
      "vpm.encoder.layers.9.mlp.fc1.weight\n",
      "vpm.encoder.layers.9.mlp.fc1.bias\n",
      "vpm.encoder.layers.9.mlp.fc2.weight\n",
      "vpm.encoder.layers.9.mlp.fc2.bias\n",
      "vpm.encoder.layers.9.layer_norm2.weight\n",
      "vpm.encoder.layers.9.layer_norm2.bias\n",
      "vpm.encoder.layers.10.self_attn.k_proj.weight\n",
      "vpm.encoder.layers.10.self_attn.k_proj.bias\n",
      "vpm.encoder.layers.10.self_attn.v_proj.weight\n",
      "vpm.encoder.layers.10.self_attn.v_proj.bias\n",
      "vpm.encoder.layers.10.self_attn.q_proj.weight\n",
      "vpm.encoder.layers.10.self_attn.q_proj.bias\n",
      "vpm.encoder.layers.10.self_attn.out_proj.weight\n",
      "vpm.encoder.layers.10.self_attn.out_proj.bias\n",
      "vpm.encoder.layers.10.layer_norm1.weight\n",
      "vpm.encoder.layers.10.layer_norm1.bias\n",
      "vpm.encoder.layers.10.mlp.fc1.weight\n",
      "vpm.encoder.layers.10.mlp.fc1.bias\n",
      "vpm.encoder.layers.10.mlp.fc2.weight\n",
      "vpm.encoder.layers.10.mlp.fc2.bias\n",
      "vpm.encoder.layers.10.layer_norm2.weight\n",
      "vpm.encoder.layers.10.layer_norm2.bias\n",
      "vpm.encoder.layers.11.self_attn.k_proj.weight\n",
      "vpm.encoder.layers.11.self_attn.k_proj.bias\n",
      "vpm.encoder.layers.11.self_attn.v_proj.weight\n",
      "vpm.encoder.layers.11.self_attn.v_proj.bias\n",
      "vpm.encoder.layers.11.self_attn.q_proj.weight\n",
      "vpm.encoder.layers.11.self_attn.q_proj.bias\n",
      "vpm.encoder.layers.11.self_attn.out_proj.weight\n",
      "vpm.encoder.layers.11.self_attn.out_proj.bias\n",
      "vpm.encoder.layers.11.layer_norm1.weight\n",
      "vpm.encoder.layers.11.layer_norm1.bias\n",
      "vpm.encoder.layers.11.mlp.fc1.weight\n",
      "vpm.encoder.layers.11.mlp.fc1.bias\n",
      "vpm.encoder.layers.11.mlp.fc2.weight\n",
      "vpm.encoder.layers.11.mlp.fc2.bias\n",
      "vpm.encoder.layers.11.layer_norm2.weight\n",
      "vpm.encoder.layers.11.layer_norm2.bias\n",
      "vpm.encoder.layers.12.self_attn.k_proj.weight\n",
      "vpm.encoder.layers.12.self_attn.k_proj.bias\n",
      "vpm.encoder.layers.12.self_attn.v_proj.weight\n",
      "vpm.encoder.layers.12.self_attn.v_proj.bias\n",
      "vpm.encoder.layers.12.self_attn.q_proj.weight\n",
      "vpm.encoder.layers.12.self_attn.q_proj.bias\n",
      "vpm.encoder.layers.12.self_attn.out_proj.weight\n",
      "vpm.encoder.layers.12.self_attn.out_proj.bias\n",
      "vpm.encoder.layers.12.layer_norm1.weight\n",
      "vpm.encoder.layers.12.layer_norm1.bias\n",
      "vpm.encoder.layers.12.mlp.fc1.weight\n",
      "vpm.encoder.layers.12.mlp.fc1.bias\n",
      "vpm.encoder.layers.12.mlp.fc2.weight\n",
      "vpm.encoder.layers.12.mlp.fc2.bias\n",
      "vpm.encoder.layers.12.layer_norm2.weight\n",
      "vpm.encoder.layers.12.layer_norm2.bias\n",
      "vpm.encoder.layers.13.self_attn.k_proj.weight\n",
      "vpm.encoder.layers.13.self_attn.k_proj.bias\n",
      "vpm.encoder.layers.13.self_attn.v_proj.weight\n",
      "vpm.encoder.layers.13.self_attn.v_proj.bias\n",
      "vpm.encoder.layers.13.self_attn.q_proj.weight\n",
      "vpm.encoder.layers.13.self_attn.q_proj.bias\n",
      "vpm.encoder.layers.13.self_attn.out_proj.weight\n",
      "vpm.encoder.layers.13.self_attn.out_proj.bias\n",
      "vpm.encoder.layers.13.layer_norm1.weight\n",
      "vpm.encoder.layers.13.layer_norm1.bias\n",
      "vpm.encoder.layers.13.mlp.fc1.weight\n",
      "vpm.encoder.layers.13.mlp.fc1.bias\n",
      "vpm.encoder.layers.13.mlp.fc2.weight\n",
      "vpm.encoder.layers.13.mlp.fc2.bias\n",
      "vpm.encoder.layers.13.layer_norm2.weight\n",
      "vpm.encoder.layers.13.layer_norm2.bias\n",
      "vpm.encoder.layers.14.self_attn.k_proj.weight\n",
      "vpm.encoder.layers.14.self_attn.k_proj.bias\n",
      "vpm.encoder.layers.14.self_attn.v_proj.weight\n",
      "vpm.encoder.layers.14.self_attn.v_proj.bias\n",
      "vpm.encoder.layers.14.self_attn.q_proj.weight\n",
      "vpm.encoder.layers.14.self_attn.q_proj.bias\n",
      "vpm.encoder.layers.14.self_attn.out_proj.weight\n",
      "vpm.encoder.layers.14.self_attn.out_proj.bias\n",
      "vpm.encoder.layers.14.layer_norm1.weight\n",
      "vpm.encoder.layers.14.layer_norm1.bias\n",
      "vpm.encoder.layers.14.mlp.fc1.weight\n",
      "vpm.encoder.layers.14.mlp.fc1.bias\n",
      "vpm.encoder.layers.14.mlp.fc2.weight\n",
      "vpm.encoder.layers.14.mlp.fc2.bias\n",
      "vpm.encoder.layers.14.layer_norm2.weight\n",
      "vpm.encoder.layers.14.layer_norm2.bias\n",
      "vpm.encoder.layers.15.self_attn.k_proj.weight\n",
      "vpm.encoder.layers.15.self_attn.k_proj.bias\n",
      "vpm.encoder.layers.15.self_attn.v_proj.weight\n",
      "vpm.encoder.layers.15.self_attn.v_proj.bias\n",
      "vpm.encoder.layers.15.self_attn.q_proj.weight\n",
      "vpm.encoder.layers.15.self_attn.q_proj.bias\n",
      "vpm.encoder.layers.15.self_attn.out_proj.weight\n",
      "vpm.encoder.layers.15.self_attn.out_proj.bias\n",
      "vpm.encoder.layers.15.layer_norm1.weight\n",
      "vpm.encoder.layers.15.layer_norm1.bias\n",
      "vpm.encoder.layers.15.mlp.fc1.weight\n",
      "vpm.encoder.layers.15.mlp.fc1.bias\n",
      "vpm.encoder.layers.15.mlp.fc2.weight\n",
      "vpm.encoder.layers.15.mlp.fc2.bias\n",
      "vpm.encoder.layers.15.layer_norm2.weight\n",
      "vpm.encoder.layers.15.layer_norm2.bias\n",
      "vpm.encoder.layers.16.self_attn.k_proj.weight\n",
      "vpm.encoder.layers.16.self_attn.k_proj.bias\n",
      "vpm.encoder.layers.16.self_attn.v_proj.weight\n",
      "vpm.encoder.layers.16.self_attn.v_proj.bias\n",
      "vpm.encoder.layers.16.self_attn.q_proj.weight\n",
      "vpm.encoder.layers.16.self_attn.q_proj.bias\n",
      "vpm.encoder.layers.16.self_attn.out_proj.weight\n",
      "vpm.encoder.layers.16.self_attn.out_proj.bias\n",
      "vpm.encoder.layers.16.layer_norm1.weight\n",
      "vpm.encoder.layers.16.layer_norm1.bias\n",
      "vpm.encoder.layers.16.mlp.fc1.weight\n",
      "vpm.encoder.layers.16.mlp.fc1.bias\n",
      "vpm.encoder.layers.16.mlp.fc2.weight\n",
      "vpm.encoder.layers.16.mlp.fc2.bias\n",
      "vpm.encoder.layers.16.layer_norm2.weight\n",
      "vpm.encoder.layers.16.layer_norm2.bias\n",
      "vpm.encoder.layers.17.self_attn.k_proj.weight\n",
      "vpm.encoder.layers.17.self_attn.k_proj.bias\n",
      "vpm.encoder.layers.17.self_attn.v_proj.weight\n",
      "vpm.encoder.layers.17.self_attn.v_proj.bias\n",
      "vpm.encoder.layers.17.self_attn.q_proj.weight\n",
      "vpm.encoder.layers.17.self_attn.q_proj.bias\n",
      "vpm.encoder.layers.17.self_attn.out_proj.weight\n",
      "vpm.encoder.layers.17.self_attn.out_proj.bias\n",
      "vpm.encoder.layers.17.layer_norm1.weight\n",
      "vpm.encoder.layers.17.layer_norm1.bias\n",
      "vpm.encoder.layers.17.mlp.fc1.weight\n",
      "vpm.encoder.layers.17.mlp.fc1.bias\n",
      "vpm.encoder.layers.17.mlp.fc2.weight\n",
      "vpm.encoder.layers.17.mlp.fc2.bias\n",
      "vpm.encoder.layers.17.layer_norm2.weight\n",
      "vpm.encoder.layers.17.layer_norm2.bias\n",
      "vpm.encoder.layers.18.self_attn.k_proj.weight\n",
      "vpm.encoder.layers.18.self_attn.k_proj.bias\n",
      "vpm.encoder.layers.18.self_attn.v_proj.weight\n",
      "vpm.encoder.layers.18.self_attn.v_proj.bias\n",
      "vpm.encoder.layers.18.self_attn.q_proj.weight\n",
      "vpm.encoder.layers.18.self_attn.q_proj.bias\n",
      "vpm.encoder.layers.18.self_attn.out_proj.weight\n",
      "vpm.encoder.layers.18.self_attn.out_proj.bias\n",
      "vpm.encoder.layers.18.layer_norm1.weight\n",
      "vpm.encoder.layers.18.layer_norm1.bias\n",
      "vpm.encoder.layers.18.mlp.fc1.weight\n",
      "vpm.encoder.layers.18.mlp.fc1.bias\n",
      "vpm.encoder.layers.18.mlp.fc2.weight\n",
      "vpm.encoder.layers.18.mlp.fc2.bias\n",
      "vpm.encoder.layers.18.layer_norm2.weight\n",
      "vpm.encoder.layers.18.layer_norm2.bias\n",
      "vpm.encoder.layers.19.self_attn.k_proj.weight\n",
      "vpm.encoder.layers.19.self_attn.k_proj.bias\n",
      "vpm.encoder.layers.19.self_attn.v_proj.weight\n",
      "vpm.encoder.layers.19.self_attn.v_proj.bias\n",
      "vpm.encoder.layers.19.self_attn.q_proj.weight\n",
      "vpm.encoder.layers.19.self_attn.q_proj.bias\n",
      "vpm.encoder.layers.19.self_attn.out_proj.weight\n",
      "vpm.encoder.layers.19.self_attn.out_proj.bias\n",
      "vpm.encoder.layers.19.layer_norm1.weight\n",
      "vpm.encoder.layers.19.layer_norm1.bias\n",
      "vpm.encoder.layers.19.mlp.fc1.weight\n",
      "vpm.encoder.layers.19.mlp.fc1.bias\n",
      "vpm.encoder.layers.19.mlp.fc2.weight\n",
      "vpm.encoder.layers.19.mlp.fc2.bias\n",
      "vpm.encoder.layers.19.layer_norm2.weight\n",
      "vpm.encoder.layers.19.layer_norm2.bias\n",
      "vpm.encoder.layers.20.self_attn.k_proj.weight\n",
      "vpm.encoder.layers.20.self_attn.k_proj.bias\n",
      "vpm.encoder.layers.20.self_attn.v_proj.weight\n",
      "vpm.encoder.layers.20.self_attn.v_proj.bias\n",
      "vpm.encoder.layers.20.self_attn.q_proj.weight\n",
      "vpm.encoder.layers.20.self_attn.q_proj.bias\n",
      "vpm.encoder.layers.20.self_attn.out_proj.weight\n",
      "vpm.encoder.layers.20.self_attn.out_proj.bias\n",
      "vpm.encoder.layers.20.layer_norm1.weight\n",
      "vpm.encoder.layers.20.layer_norm1.bias\n",
      "vpm.encoder.layers.20.mlp.fc1.weight\n",
      "vpm.encoder.layers.20.mlp.fc1.bias\n",
      "vpm.encoder.layers.20.mlp.fc2.weight\n",
      "vpm.encoder.layers.20.mlp.fc2.bias\n",
      "vpm.encoder.layers.20.layer_norm2.weight\n",
      "vpm.encoder.layers.20.layer_norm2.bias\n",
      "vpm.encoder.layers.21.self_attn.k_proj.weight\n",
      "vpm.encoder.layers.21.self_attn.k_proj.bias\n",
      "vpm.encoder.layers.21.self_attn.v_proj.weight\n",
      "vpm.encoder.layers.21.self_attn.v_proj.bias\n",
      "vpm.encoder.layers.21.self_attn.q_proj.weight\n",
      "vpm.encoder.layers.21.self_attn.q_proj.bias\n",
      "vpm.encoder.layers.21.self_attn.out_proj.weight\n",
      "vpm.encoder.layers.21.self_attn.out_proj.bias\n",
      "vpm.encoder.layers.21.layer_norm1.weight\n",
      "vpm.encoder.layers.21.layer_norm1.bias\n",
      "vpm.encoder.layers.21.mlp.fc1.weight\n",
      "vpm.encoder.layers.21.mlp.fc1.bias\n",
      "vpm.encoder.layers.21.mlp.fc2.weight\n",
      "vpm.encoder.layers.21.mlp.fc2.bias\n",
      "vpm.encoder.layers.21.layer_norm2.weight\n",
      "vpm.encoder.layers.21.layer_norm2.bias\n",
      "vpm.encoder.layers.22.self_attn.k_proj.weight\n",
      "vpm.encoder.layers.22.self_attn.k_proj.bias\n",
      "vpm.encoder.layers.22.self_attn.v_proj.weight\n",
      "vpm.encoder.layers.22.self_attn.v_proj.bias\n",
      "vpm.encoder.layers.22.self_attn.q_proj.weight\n",
      "vpm.encoder.layers.22.self_attn.q_proj.bias\n",
      "vpm.encoder.layers.22.self_attn.out_proj.weight\n",
      "vpm.encoder.layers.22.self_attn.out_proj.bias\n",
      "vpm.encoder.layers.22.layer_norm1.weight\n",
      "vpm.encoder.layers.22.layer_norm1.bias\n",
      "vpm.encoder.layers.22.mlp.fc1.weight\n",
      "vpm.encoder.layers.22.mlp.fc1.bias\n",
      "vpm.encoder.layers.22.mlp.fc2.weight\n",
      "vpm.encoder.layers.22.mlp.fc2.bias\n",
      "vpm.encoder.layers.22.layer_norm2.weight\n",
      "vpm.encoder.layers.22.layer_norm2.bias\n",
      "vpm.encoder.layers.23.self_attn.k_proj.weight\n",
      "vpm.encoder.layers.23.self_attn.k_proj.bias\n",
      "vpm.encoder.layers.23.self_attn.v_proj.weight\n",
      "vpm.encoder.layers.23.self_attn.v_proj.bias\n",
      "vpm.encoder.layers.23.self_attn.q_proj.weight\n",
      "vpm.encoder.layers.23.self_attn.q_proj.bias\n",
      "vpm.encoder.layers.23.self_attn.out_proj.weight\n",
      "vpm.encoder.layers.23.self_attn.out_proj.bias\n",
      "vpm.encoder.layers.23.layer_norm1.weight\n",
      "vpm.encoder.layers.23.layer_norm1.bias\n",
      "vpm.encoder.layers.23.mlp.fc1.weight\n",
      "vpm.encoder.layers.23.mlp.fc1.bias\n",
      "vpm.encoder.layers.23.mlp.fc2.weight\n",
      "vpm.encoder.layers.23.mlp.fc2.bias\n",
      "vpm.encoder.layers.23.layer_norm2.weight\n",
      "vpm.encoder.layers.23.layer_norm2.bias\n",
      "vpm.encoder.layers.24.self_attn.k_proj.weight\n",
      "vpm.encoder.layers.24.self_attn.k_proj.bias\n",
      "vpm.encoder.layers.24.self_attn.v_proj.weight\n",
      "vpm.encoder.layers.24.self_attn.v_proj.bias\n",
      "vpm.encoder.layers.24.self_attn.q_proj.weight\n",
      "vpm.encoder.layers.24.self_attn.q_proj.bias\n",
      "vpm.encoder.layers.24.self_attn.out_proj.weight\n",
      "vpm.encoder.layers.24.self_attn.out_proj.bias\n",
      "vpm.encoder.layers.24.layer_norm1.weight\n",
      "vpm.encoder.layers.24.layer_norm1.bias\n",
      "vpm.encoder.layers.24.mlp.fc1.weight\n",
      "vpm.encoder.layers.24.mlp.fc1.bias\n",
      "vpm.encoder.layers.24.mlp.fc2.weight\n",
      "vpm.encoder.layers.24.mlp.fc2.bias\n",
      "vpm.encoder.layers.24.layer_norm2.weight\n",
      "vpm.encoder.layers.24.layer_norm2.bias\n",
      "vpm.encoder.layers.25.self_attn.k_proj.weight\n",
      "vpm.encoder.layers.25.self_attn.k_proj.bias\n",
      "vpm.encoder.layers.25.self_attn.v_proj.weight\n",
      "vpm.encoder.layers.25.self_attn.v_proj.bias\n",
      "vpm.encoder.layers.25.self_attn.q_proj.weight\n",
      "vpm.encoder.layers.25.self_attn.q_proj.bias\n",
      "vpm.encoder.layers.25.self_attn.out_proj.weight\n",
      "vpm.encoder.layers.25.self_attn.out_proj.bias\n",
      "vpm.encoder.layers.25.layer_norm1.weight\n",
      "vpm.encoder.layers.25.layer_norm1.bias\n",
      "vpm.encoder.layers.25.mlp.fc1.weight\n",
      "vpm.encoder.layers.25.mlp.fc1.bias\n",
      "vpm.encoder.layers.25.mlp.fc2.weight\n",
      "vpm.encoder.layers.25.mlp.fc2.bias\n",
      "vpm.encoder.layers.25.layer_norm2.weight\n",
      "vpm.encoder.layers.25.layer_norm2.bias\n",
      "vpm.encoder.layers.26.self_attn.k_proj.weight\n",
      "vpm.encoder.layers.26.self_attn.k_proj.bias\n",
      "vpm.encoder.layers.26.self_attn.v_proj.weight\n",
      "vpm.encoder.layers.26.self_attn.v_proj.bias\n",
      "vpm.encoder.layers.26.self_attn.q_proj.weight\n",
      "vpm.encoder.layers.26.self_attn.q_proj.bias\n",
      "vpm.encoder.layers.26.self_attn.out_proj.weight\n",
      "vpm.encoder.layers.26.self_attn.out_proj.bias\n",
      "vpm.encoder.layers.26.layer_norm1.weight\n",
      "vpm.encoder.layers.26.layer_norm1.bias\n",
      "vpm.encoder.layers.26.mlp.fc1.weight\n",
      "vpm.encoder.layers.26.mlp.fc1.bias\n",
      "vpm.encoder.layers.26.mlp.fc2.weight\n",
      "vpm.encoder.layers.26.mlp.fc2.bias\n",
      "vpm.encoder.layers.26.layer_norm2.weight\n",
      "vpm.encoder.layers.26.layer_norm2.bias\n",
      "vpm.post_layernorm.weight\n",
      "vpm.post_layernorm.bias\n",
      "resampler.query\n",
      "resampler.proj\n",
      "resampler.kv_proj.weight\n",
      "resampler.attn.in_proj_weight\n",
      "resampler.attn.in_proj_bias\n",
      "resampler.attn.out_proj.weight\n",
      "resampler.attn.out_proj.bias\n",
      "resampler.ln_q.weight\n",
      "resampler.ln_q.bias\n",
      "resampler.ln_kv.weight\n",
      "resampler.ln_kv.bias\n",
      "resampler.ln_post.weight\n",
      "resampler.ln_post.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# find the model part that includes the the multimodal projector weights\n",
    "model = AutoModel.from_pretrained('/media/m3/models/MiniCPM-V-2_6', trust_remote_code=True, ignore_mismatched_sizes=True)\n",
    "checkpoint = model.state_dict()\n",
    "\n",
    "for k, v in checkpoint.items():\n",
    "    print(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "./build/bin/llama-minicpmv-cli -m /media/m3/models/MiniCPM-V-2_6/model/Model-7.6B-F16.gguf --mmproj /media/m3/models/MiniCPM-V-2_6/mmproj-model-f16.gguf --image tmp.jpg -c 4096 --temp 0.7 --top-p 0.8 --top-k 100 --repeat-penalty 1.05 -p \"What is in the image?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # get a list of mm tensor names\n",
    "# mm_tensors = [k for k, v in checkpoint.items() if k.startswith(\"resampler\")]\n",
    "\n",
    "# # store these tensors in a new dictionary and torch.save them\n",
    "# projector = {name: checkpoint[name].float() for name in mm_tensors}\n",
    "# torch.save(projector, f\"{args.model}/minicpmv.projector\")\n",
    "\n",
    "# clip_tensors = [k for k, v in checkpoint.items() if k.startswith(\"vpm\")]\n",
    "# if len(clip_tensors) > 0:\n",
    "#     clip = {name.replace(\"vpm.\", \"\"): checkpoint[name].float() for name in clip_tensors}\n",
    "#     torch.save(clip, f\"{args.model}/minicpmv.clip\")\n",
    "\n",
    "#     # added tokens should be removed to be able to convert Mistral models\n",
    "#     if os.path.exists(f\"{args.model}/added_tokens.json\"):\n",
    "#         with open(f\"{args.model}/added_tokens.json\", \"w\") as f:\n",
    "#             f.write(\"{}\\n\")\n",
    "\n",
    "# config = model.llm.config\n",
    "# config.auto_map = {\n",
    "#     \"AutoConfig\": \"configuration_minicpm.MiniCPMConfig\",\n",
    "#     \"AutoModel\": \"modeling_minicpm.MiniCPMModel\",\n",
    "#     \"AutoModelForCausalLM\": \"modeling_minicpm.MiniCPMForCausalLM\",\n",
    "#     \"AutoModelForSeq2SeqLM\": \"modeling_minicpm.MiniCPMForCausalLM\",\n",
    "#     \"AutoModelForSequenceClassification\": \"modeling_minicpm.MiniCPMForSequenceClassification\"\n",
    "# }\n",
    "# model.llm.save_pretrained(f\"{args.model}/model\")\n",
    "# tok = AutoTokenizer.from_pretrained(args.model, trust_remote_code=True)\n",
    "# tok.save_pretrained(f\"{args.model}/model\")\n",
    "\n",
    "# print(\"Done!\")\n",
    "# print(f\"Now you can convert {args.model} to a regular LLaMA GGUF file.\")\n",
    "# print(f\"Also, use {args.model}/minicpmv.projector to prepare a minicpmv-encoder.gguf file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mm.0.weight [1280 4096]\n",
      "mm.0.bias [4096]\n",
      "mm.1.weight [4096]\n",
      "mm.1.bias [4096]\n",
      "mm.3.weight [4096 4096]\n",
      "mm.3.bias [4096]\n",
      "mm.4.weight [4096]\n",
      "mm.4.bias [4096]\n",
      "v.class_embd [1280]\n",
      "v.patch_embd.weight [  14   14    3 1280]\n",
      "v.position_embd.weight [1280 1025]\n",
      "v.pre_ln.weight [1280]\n",
      "v.pre_ln.bias [1280]\n",
      "v.blk.0.attn_k.weight [1280 1280]\n",
      "v.blk.0.attn_k.bias [1280]\n",
      "v.blk.0.attn_v.weight [1280 1280]\n",
      "v.blk.0.attn_v.bias [1280]\n",
      "v.blk.0.attn_q.weight [1280 1280]\n",
      "v.blk.0.attn_q.bias [1280]\n",
      "v.blk.0.attn_out.weight [1280 1280]\n",
      "v.blk.0.attn_out.bias [1280]\n",
      "v.blk.0.ln1.weight [1280]\n",
      "v.blk.0.ln1.bias [1280]\n",
      "v.blk.0.ffn_down.weight [1280 5120]\n",
      "v.blk.0.ffn_down.bias [5120]\n",
      "v.blk.0.ffn_up.weight [5120 1280]\n",
      "v.blk.0.ffn_up.bias [1280]\n",
      "v.blk.0.ln2.weight [1280]\n",
      "v.blk.0.ln2.bias [1280]\n",
      "v.blk.1.attn_k.weight [1280 1280]\n",
      "v.blk.1.attn_k.bias [1280]\n",
      "v.blk.1.attn_v.weight [1280 1280]\n",
      "v.blk.1.attn_v.bias [1280]\n",
      "v.blk.1.attn_q.weight [1280 1280]\n",
      "v.blk.1.attn_q.bias [1280]\n",
      "v.blk.1.attn_out.weight [1280 1280]\n",
      "v.blk.1.attn_out.bias [1280]\n",
      "v.blk.1.ln1.weight [1280]\n",
      "v.blk.1.ln1.bias [1280]\n",
      "v.blk.1.ffn_down.weight [1280 5120]\n",
      "v.blk.1.ffn_down.bias [5120]\n",
      "v.blk.1.ffn_up.weight [5120 1280]\n",
      "v.blk.1.ffn_up.bias [1280]\n",
      "v.blk.1.ln2.weight [1280]\n",
      "v.blk.1.ln2.bias [1280]\n",
      "v.blk.2.attn_k.weight [1280 1280]\n",
      "v.blk.2.attn_k.bias [1280]\n",
      "v.blk.2.attn_v.weight [1280 1280]\n",
      "v.blk.2.attn_v.bias [1280]\n",
      "v.blk.2.attn_q.weight [1280 1280]\n",
      "v.blk.2.attn_q.bias [1280]\n",
      "v.blk.2.attn_out.weight [1280 1280]\n",
      "v.blk.2.attn_out.bias [1280]\n",
      "v.blk.2.ln1.weight [1280]\n",
      "v.blk.2.ln1.bias [1280]\n",
      "v.blk.2.ffn_down.weight [1280 5120]\n",
      "v.blk.2.ffn_down.bias [5120]\n",
      "v.blk.2.ffn_up.weight [5120 1280]\n",
      "v.blk.2.ffn_up.bias [1280]\n",
      "v.blk.2.ln2.weight [1280]\n",
      "v.blk.2.ln2.bias [1280]\n",
      "v.blk.3.attn_k.weight [1280 1280]\n",
      "v.blk.3.attn_k.bias [1280]\n",
      "v.blk.3.attn_v.weight [1280 1280]\n",
      "v.blk.3.attn_v.bias [1280]\n",
      "v.blk.3.attn_q.weight [1280 1280]\n",
      "v.blk.3.attn_q.bias [1280]\n",
      "v.blk.3.attn_out.weight [1280 1280]\n",
      "v.blk.3.attn_out.bias [1280]\n",
      "v.blk.3.ln1.weight [1280]\n",
      "v.blk.3.ln1.bias [1280]\n",
      "v.blk.3.ffn_down.weight [1280 5120]\n",
      "v.blk.3.ffn_down.bias [5120]\n",
      "v.blk.3.ffn_up.weight [5120 1280]\n",
      "v.blk.3.ffn_up.bias [1280]\n",
      "v.blk.3.ln2.weight [1280]\n",
      "v.blk.3.ln2.bias [1280]\n",
      "v.blk.4.attn_k.weight [1280 1280]\n",
      "v.blk.4.attn_k.bias [1280]\n",
      "v.blk.4.attn_v.weight [1280 1280]\n",
      "v.blk.4.attn_v.bias [1280]\n",
      "v.blk.4.attn_q.weight [1280 1280]\n",
      "v.blk.4.attn_q.bias [1280]\n",
      "v.blk.4.attn_out.weight [1280 1280]\n",
      "v.blk.4.attn_out.bias [1280]\n",
      "v.blk.4.ln1.weight [1280]\n",
      "v.blk.4.ln1.bias [1280]\n",
      "v.blk.4.ffn_down.weight [1280 5120]\n",
      "v.blk.4.ffn_down.bias [5120]\n",
      "v.blk.4.ffn_up.weight [5120 1280]\n",
      "v.blk.4.ffn_up.bias [1280]\n",
      "v.blk.4.ln2.weight [1280]\n",
      "v.blk.4.ln2.bias [1280]\n",
      "v.blk.5.attn_k.weight [1280 1280]\n",
      "v.blk.5.attn_k.bias [1280]\n",
      "v.blk.5.attn_v.weight [1280 1280]\n",
      "v.blk.5.attn_v.bias [1280]\n",
      "v.blk.5.attn_q.weight [1280 1280]\n",
      "v.blk.5.attn_q.bias [1280]\n",
      "v.blk.5.attn_out.weight [1280 1280]\n",
      "v.blk.5.attn_out.bias [1280]\n",
      "v.blk.5.ln1.weight [1280]\n",
      "v.blk.5.ln1.bias [1280]\n",
      "v.blk.5.ffn_down.weight [1280 5120]\n",
      "v.blk.5.ffn_down.bias [5120]\n",
      "v.blk.5.ffn_up.weight [5120 1280]\n",
      "v.blk.5.ffn_up.bias [1280]\n",
      "v.blk.5.ln2.weight [1280]\n",
      "v.blk.5.ln2.bias [1280]\n",
      "v.blk.6.attn_k.weight [1280 1280]\n",
      "v.blk.6.attn_k.bias [1280]\n",
      "v.blk.6.attn_v.weight [1280 1280]\n",
      "v.blk.6.attn_v.bias [1280]\n",
      "v.blk.6.attn_q.weight [1280 1280]\n",
      "v.blk.6.attn_q.bias [1280]\n",
      "v.blk.6.attn_out.weight [1280 1280]\n",
      "v.blk.6.attn_out.bias [1280]\n",
      "v.blk.6.ln1.weight [1280]\n",
      "v.blk.6.ln1.bias [1280]\n",
      "v.blk.6.ffn_down.weight [1280 5120]\n",
      "v.blk.6.ffn_down.bias [5120]\n",
      "v.blk.6.ffn_up.weight [5120 1280]\n",
      "v.blk.6.ffn_up.bias [1280]\n",
      "v.blk.6.ln2.weight [1280]\n",
      "v.blk.6.ln2.bias [1280]\n",
      "v.blk.7.attn_k.weight [1280 1280]\n",
      "v.blk.7.attn_k.bias [1280]\n",
      "v.blk.7.attn_v.weight [1280 1280]\n",
      "v.blk.7.attn_v.bias [1280]\n",
      "v.blk.7.attn_q.weight [1280 1280]\n",
      "v.blk.7.attn_q.bias [1280]\n",
      "v.blk.7.attn_out.weight [1280 1280]\n",
      "v.blk.7.attn_out.bias [1280]\n",
      "v.blk.7.ln1.weight [1280]\n",
      "v.blk.7.ln1.bias [1280]\n",
      "v.blk.7.ffn_down.weight [1280 5120]\n",
      "v.blk.7.ffn_down.bias [5120]\n",
      "v.blk.7.ffn_up.weight [5120 1280]\n",
      "v.blk.7.ffn_up.bias [1280]\n",
      "v.blk.7.ln2.weight [1280]\n",
      "v.blk.7.ln2.bias [1280]\n",
      "v.blk.8.attn_k.weight [1280 1280]\n",
      "v.blk.8.attn_k.bias [1280]\n",
      "v.blk.8.attn_v.weight [1280 1280]\n",
      "v.blk.8.attn_v.bias [1280]\n",
      "v.blk.8.attn_q.weight [1280 1280]\n",
      "v.blk.8.attn_q.bias [1280]\n",
      "v.blk.8.attn_out.weight [1280 1280]\n",
      "v.blk.8.attn_out.bias [1280]\n",
      "v.blk.8.ln1.weight [1280]\n",
      "v.blk.8.ln1.bias [1280]\n",
      "v.blk.8.ffn_down.weight [1280 5120]\n",
      "v.blk.8.ffn_down.bias [5120]\n",
      "v.blk.8.ffn_up.weight [5120 1280]\n",
      "v.blk.8.ffn_up.bias [1280]\n",
      "v.blk.8.ln2.weight [1280]\n",
      "v.blk.8.ln2.bias [1280]\n",
      "v.blk.9.attn_k.weight [1280 1280]\n",
      "v.blk.9.attn_k.bias [1280]\n",
      "v.blk.9.attn_v.weight [1280 1280]\n",
      "v.blk.9.attn_v.bias [1280]\n",
      "v.blk.9.attn_q.weight [1280 1280]\n",
      "v.blk.9.attn_q.bias [1280]\n",
      "v.blk.9.attn_out.weight [1280 1280]\n",
      "v.blk.9.attn_out.bias [1280]\n",
      "v.blk.9.ln1.weight [1280]\n",
      "v.blk.9.ln1.bias [1280]\n",
      "v.blk.9.ffn_down.weight [1280 5120]\n",
      "v.blk.9.ffn_down.bias [5120]\n",
      "v.blk.9.ffn_up.weight [5120 1280]\n",
      "v.blk.9.ffn_up.bias [1280]\n",
      "v.blk.9.ln2.weight [1280]\n",
      "v.blk.9.ln2.bias [1280]\n",
      "v.blk.10.attn_k.weight [1280 1280]\n",
      "v.blk.10.attn_k.bias [1280]\n",
      "v.blk.10.attn_v.weight [1280 1280]\n",
      "v.blk.10.attn_v.bias [1280]\n",
      "v.blk.10.attn_q.weight [1280 1280]\n",
      "v.blk.10.attn_q.bias [1280]\n",
      "v.blk.10.attn_out.weight [1280 1280]\n",
      "v.blk.10.attn_out.bias [1280]\n",
      "v.blk.10.ln1.weight [1280]\n",
      "v.blk.10.ln1.bias [1280]\n",
      "v.blk.10.ffn_down.weight [1280 5120]\n",
      "v.blk.10.ffn_down.bias [5120]\n",
      "v.blk.10.ffn_up.weight [5120 1280]\n",
      "v.blk.10.ffn_up.bias [1280]\n",
      "v.blk.10.ln2.weight [1280]\n",
      "v.blk.10.ln2.bias [1280]\n",
      "v.blk.11.attn_k.weight [1280 1280]\n",
      "v.blk.11.attn_k.bias [1280]\n",
      "v.blk.11.attn_v.weight [1280 1280]\n",
      "v.blk.11.attn_v.bias [1280]\n",
      "v.blk.11.attn_q.weight [1280 1280]\n",
      "v.blk.11.attn_q.bias [1280]\n",
      "v.blk.11.attn_out.weight [1280 1280]\n",
      "v.blk.11.attn_out.bias [1280]\n",
      "v.blk.11.ln1.weight [1280]\n",
      "v.blk.11.ln1.bias [1280]\n",
      "v.blk.11.ffn_down.weight [1280 5120]\n",
      "v.blk.11.ffn_down.bias [5120]\n",
      "v.blk.11.ffn_up.weight [5120 1280]\n",
      "v.blk.11.ffn_up.bias [1280]\n",
      "v.blk.11.ln2.weight [1280]\n",
      "v.blk.11.ln2.bias [1280]\n",
      "v.blk.12.attn_k.weight [1280 1280]\n",
      "v.blk.12.attn_k.bias [1280]\n",
      "v.blk.12.attn_v.weight [1280 1280]\n",
      "v.blk.12.attn_v.bias [1280]\n",
      "v.blk.12.attn_q.weight [1280 1280]\n",
      "v.blk.12.attn_q.bias [1280]\n",
      "v.blk.12.attn_out.weight [1280 1280]\n",
      "v.blk.12.attn_out.bias [1280]\n",
      "v.blk.12.ln1.weight [1280]\n",
      "v.blk.12.ln1.bias [1280]\n",
      "v.blk.12.ffn_down.weight [1280 5120]\n",
      "v.blk.12.ffn_down.bias [5120]\n",
      "v.blk.12.ffn_up.weight [5120 1280]\n",
      "v.blk.12.ffn_up.bias [1280]\n",
      "v.blk.12.ln2.weight [1280]\n",
      "v.blk.12.ln2.bias [1280]\n",
      "v.blk.13.attn_k.weight [1280 1280]\n",
      "v.blk.13.attn_k.bias [1280]\n",
      "v.blk.13.attn_v.weight [1280 1280]\n",
      "v.blk.13.attn_v.bias [1280]\n",
      "v.blk.13.attn_q.weight [1280 1280]\n",
      "v.blk.13.attn_q.bias [1280]\n",
      "v.blk.13.attn_out.weight [1280 1280]\n",
      "v.blk.13.attn_out.bias [1280]\n",
      "v.blk.13.ln1.weight [1280]\n",
      "v.blk.13.ln1.bias [1280]\n",
      "v.blk.13.ffn_down.weight [1280 5120]\n",
      "v.blk.13.ffn_down.bias [5120]\n",
      "v.blk.13.ffn_up.weight [5120 1280]\n",
      "v.blk.13.ffn_up.bias [1280]\n",
      "v.blk.13.ln2.weight [1280]\n",
      "v.blk.13.ln2.bias [1280]\n",
      "v.blk.14.attn_k.weight [1280 1280]\n",
      "v.blk.14.attn_k.bias [1280]\n",
      "v.blk.14.attn_v.weight [1280 1280]\n",
      "v.blk.14.attn_v.bias [1280]\n",
      "v.blk.14.attn_q.weight [1280 1280]\n",
      "v.blk.14.attn_q.bias [1280]\n",
      "v.blk.14.attn_out.weight [1280 1280]\n",
      "v.blk.14.attn_out.bias [1280]\n",
      "v.blk.14.ln1.weight [1280]\n",
      "v.blk.14.ln1.bias [1280]\n",
      "v.blk.14.ffn_down.weight [1280 5120]\n",
      "v.blk.14.ffn_down.bias [5120]\n",
      "v.blk.14.ffn_up.weight [5120 1280]\n",
      "v.blk.14.ffn_up.bias [1280]\n",
      "v.blk.14.ln2.weight [1280]\n",
      "v.blk.14.ln2.bias [1280]\n",
      "v.blk.15.attn_k.weight [1280 1280]\n",
      "v.blk.15.attn_k.bias [1280]\n",
      "v.blk.15.attn_v.weight [1280 1280]\n",
      "v.blk.15.attn_v.bias [1280]\n",
      "v.blk.15.attn_q.weight [1280 1280]\n",
      "v.blk.15.attn_q.bias [1280]\n",
      "v.blk.15.attn_out.weight [1280 1280]\n",
      "v.blk.15.attn_out.bias [1280]\n",
      "v.blk.15.ln1.weight [1280]\n",
      "v.blk.15.ln1.bias [1280]\n",
      "v.blk.15.ffn_down.weight [1280 5120]\n",
      "v.blk.15.ffn_down.bias [5120]\n",
      "v.blk.15.ffn_up.weight [5120 1280]\n",
      "v.blk.15.ffn_up.bias [1280]\n",
      "v.blk.15.ln2.weight [1280]\n",
      "v.blk.15.ln2.bias [1280]\n",
      "v.blk.16.attn_k.weight [1280 1280]\n",
      "v.blk.16.attn_k.bias [1280]\n",
      "v.blk.16.attn_v.weight [1280 1280]\n",
      "v.blk.16.attn_v.bias [1280]\n",
      "v.blk.16.attn_q.weight [1280 1280]\n",
      "v.blk.16.attn_q.bias [1280]\n",
      "v.blk.16.attn_out.weight [1280 1280]\n",
      "v.blk.16.attn_out.bias [1280]\n",
      "v.blk.16.ln1.weight [1280]\n",
      "v.blk.16.ln1.bias [1280]\n",
      "v.blk.16.ffn_down.weight [1280 5120]\n",
      "v.blk.16.ffn_down.bias [5120]\n",
      "v.blk.16.ffn_up.weight [5120 1280]\n",
      "v.blk.16.ffn_up.bias [1280]\n",
      "v.blk.16.ln2.weight [1280]\n",
      "v.blk.16.ln2.bias [1280]\n",
      "v.blk.17.attn_k.weight [1280 1280]\n",
      "v.blk.17.attn_k.bias [1280]\n",
      "v.blk.17.attn_v.weight [1280 1280]\n",
      "v.blk.17.attn_v.bias [1280]\n",
      "v.blk.17.attn_q.weight [1280 1280]\n",
      "v.blk.17.attn_q.bias [1280]\n",
      "v.blk.17.attn_out.weight [1280 1280]\n",
      "v.blk.17.attn_out.bias [1280]\n",
      "v.blk.17.ln1.weight [1280]\n",
      "v.blk.17.ln1.bias [1280]\n",
      "v.blk.17.ffn_down.weight [1280 5120]\n",
      "v.blk.17.ffn_down.bias [5120]\n",
      "v.blk.17.ffn_up.weight [5120 1280]\n",
      "v.blk.17.ffn_up.bias [1280]\n",
      "v.blk.17.ln2.weight [1280]\n",
      "v.blk.17.ln2.bias [1280]\n",
      "v.blk.18.attn_k.weight [1280 1280]\n",
      "v.blk.18.attn_k.bias [1280]\n",
      "v.blk.18.attn_v.weight [1280 1280]\n",
      "v.blk.18.attn_v.bias [1280]\n",
      "v.blk.18.attn_q.weight [1280 1280]\n",
      "v.blk.18.attn_q.bias [1280]\n",
      "v.blk.18.attn_out.weight [1280 1280]\n",
      "v.blk.18.attn_out.bias [1280]\n",
      "v.blk.18.ln1.weight [1280]\n",
      "v.blk.18.ln1.bias [1280]\n",
      "v.blk.18.ffn_down.weight [1280 5120]\n",
      "v.blk.18.ffn_down.bias [5120]\n",
      "v.blk.18.ffn_up.weight [5120 1280]\n",
      "v.blk.18.ffn_up.bias [1280]\n",
      "v.blk.18.ln2.weight [1280]\n",
      "v.blk.18.ln2.bias [1280]\n",
      "v.blk.19.attn_k.weight [1280 1280]\n",
      "v.blk.19.attn_k.bias [1280]\n",
      "v.blk.19.attn_v.weight [1280 1280]\n",
      "v.blk.19.attn_v.bias [1280]\n",
      "v.blk.19.attn_q.weight [1280 1280]\n",
      "v.blk.19.attn_q.bias [1280]\n",
      "v.blk.19.attn_out.weight [1280 1280]\n",
      "v.blk.19.attn_out.bias [1280]\n",
      "v.blk.19.ln1.weight [1280]\n",
      "v.blk.19.ln1.bias [1280]\n",
      "v.blk.19.ffn_down.weight [1280 5120]\n",
      "v.blk.19.ffn_down.bias [5120]\n",
      "v.blk.19.ffn_up.weight [5120 1280]\n",
      "v.blk.19.ffn_up.bias [1280]\n",
      "v.blk.19.ln2.weight [1280]\n",
      "v.blk.19.ln2.bias [1280]\n",
      "v.blk.20.attn_k.weight [1280 1280]\n",
      "v.blk.20.attn_k.bias [1280]\n",
      "v.blk.20.attn_v.weight [1280 1280]\n",
      "v.blk.20.attn_v.bias [1280]\n",
      "v.blk.20.attn_q.weight [1280 1280]\n",
      "v.blk.20.attn_q.bias [1280]\n",
      "v.blk.20.attn_out.weight [1280 1280]\n",
      "v.blk.20.attn_out.bias [1280]\n",
      "v.blk.20.ln1.weight [1280]\n",
      "v.blk.20.ln1.bias [1280]\n",
      "v.blk.20.ffn_down.weight [1280 5120]\n",
      "v.blk.20.ffn_down.bias [5120]\n",
      "v.blk.20.ffn_up.weight [5120 1280]\n",
      "v.blk.20.ffn_up.bias [1280]\n",
      "v.blk.20.ln2.weight [1280]\n",
      "v.blk.20.ln2.bias [1280]\n",
      "v.blk.21.attn_k.weight [1280 1280]\n",
      "v.blk.21.attn_k.bias [1280]\n",
      "v.blk.21.attn_v.weight [1280 1280]\n",
      "v.blk.21.attn_v.bias [1280]\n",
      "v.blk.21.attn_q.weight [1280 1280]\n",
      "v.blk.21.attn_q.bias [1280]\n",
      "v.blk.21.attn_out.weight [1280 1280]\n",
      "v.blk.21.attn_out.bias [1280]\n",
      "v.blk.21.ln1.weight [1280]\n",
      "v.blk.21.ln1.bias [1280]\n",
      "v.blk.21.ffn_down.weight [1280 5120]\n",
      "v.blk.21.ffn_down.bias [5120]\n",
      "v.blk.21.ffn_up.weight [5120 1280]\n",
      "v.blk.21.ffn_up.bias [1280]\n",
      "v.blk.21.ln2.weight [1280]\n",
      "v.blk.21.ln2.bias [1280]\n",
      "v.blk.22.attn_k.weight [1280 1280]\n",
      "v.blk.22.attn_k.bias [1280]\n",
      "v.blk.22.attn_v.weight [1280 1280]\n",
      "v.blk.22.attn_v.bias [1280]\n",
      "v.blk.22.attn_q.weight [1280 1280]\n",
      "v.blk.22.attn_q.bias [1280]\n",
      "v.blk.22.attn_out.weight [1280 1280]\n",
      "v.blk.22.attn_out.bias [1280]\n",
      "v.blk.22.ln1.weight [1280]\n",
      "v.blk.22.ln1.bias [1280]\n",
      "v.blk.22.ffn_down.weight [1280 5120]\n",
      "v.blk.22.ffn_down.bias [5120]\n",
      "v.blk.22.ffn_up.weight [5120 1280]\n",
      "v.blk.22.ffn_up.bias [1280]\n",
      "v.blk.22.ln2.weight [1280]\n",
      "v.blk.22.ln2.bias [1280]\n",
      "v.blk.23.attn_k.weight [1280 1280]\n",
      "v.blk.23.attn_k.bias [1280]\n",
      "v.blk.23.attn_v.weight [1280 1280]\n",
      "v.blk.23.attn_v.bias [1280]\n",
      "v.blk.23.attn_q.weight [1280 1280]\n",
      "v.blk.23.attn_q.bias [1280]\n",
      "v.blk.23.attn_out.weight [1280 1280]\n",
      "v.blk.23.attn_out.bias [1280]\n",
      "v.blk.23.ln1.weight [1280]\n",
      "v.blk.23.ln1.bias [1280]\n",
      "v.blk.23.ffn_down.weight [1280 5120]\n",
      "v.blk.23.ffn_down.bias [5120]\n",
      "v.blk.23.ffn_up.weight [5120 1280]\n",
      "v.blk.23.ffn_up.bias [1280]\n",
      "v.blk.23.ln2.weight [1280]\n",
      "v.blk.23.ln2.bias [1280]\n",
      "v.blk.24.attn_k.weight [1280 1280]\n",
      "v.blk.24.attn_k.bias [1280]\n",
      "v.blk.24.attn_v.weight [1280 1280]\n",
      "v.blk.24.attn_v.bias [1280]\n",
      "v.blk.24.attn_q.weight [1280 1280]\n",
      "v.blk.24.attn_q.bias [1280]\n",
      "v.blk.24.attn_out.weight [1280 1280]\n",
      "v.blk.24.attn_out.bias [1280]\n",
      "v.blk.24.ln1.weight [1280]\n",
      "v.blk.24.ln1.bias [1280]\n",
      "v.blk.24.ffn_down.weight [1280 5120]\n",
      "v.blk.24.ffn_down.bias [5120]\n",
      "v.blk.24.ffn_up.weight [5120 1280]\n",
      "v.blk.24.ffn_up.bias [1280]\n",
      "v.blk.24.ln2.weight [1280]\n",
      "v.blk.24.ln2.bias [1280]\n",
      "v.blk.25.attn_k.weight [1280 1280]\n",
      "v.blk.25.attn_k.bias [1280]\n",
      "v.blk.25.attn_v.weight [1280 1280]\n",
      "v.blk.25.attn_v.bias [1280]\n",
      "v.blk.25.attn_q.weight [1280 1280]\n",
      "v.blk.25.attn_q.bias [1280]\n",
      "v.blk.25.attn_out.weight [1280 1280]\n",
      "v.blk.25.attn_out.bias [1280]\n",
      "v.blk.25.ln1.weight [1280]\n",
      "v.blk.25.ln1.bias [1280]\n",
      "v.blk.25.ffn_down.weight [1280 5120]\n",
      "v.blk.25.ffn_down.bias [5120]\n",
      "v.blk.25.ffn_up.weight [5120 1280]\n",
      "v.blk.25.ffn_up.bias [1280]\n",
      "v.blk.25.ln2.weight [1280]\n",
      "v.blk.25.ln2.bias [1280]\n",
      "v.blk.26.attn_k.weight [1280 1280]\n",
      "v.blk.26.attn_k.bias [1280]\n",
      "v.blk.26.attn_v.weight [1280 1280]\n",
      "v.blk.26.attn_v.bias [1280]\n",
      "v.blk.26.attn_q.weight [1280 1280]\n",
      "v.blk.26.attn_q.bias [1280]\n",
      "v.blk.26.attn_out.weight [1280 1280]\n",
      "v.blk.26.attn_out.bias [1280]\n",
      "v.blk.26.ln1.weight [1280]\n",
      "v.blk.26.ln1.bias [1280]\n",
      "v.blk.26.ffn_down.weight [1280 5120]\n",
      "v.blk.26.ffn_down.bias [5120]\n",
      "v.blk.26.ffn_up.weight [5120 1280]\n",
      "v.blk.26.ffn_up.bias [1280]\n",
      "v.blk.26.ln2.weight [1280]\n",
      "v.blk.26.ln2.bias [1280]\n",
      "v.blk.27.attn_k.weight [1280 1280]\n",
      "v.blk.27.attn_k.bias [1280]\n",
      "v.blk.27.attn_v.weight [1280 1280]\n",
      "v.blk.27.attn_v.bias [1280]\n",
      "v.blk.27.attn_q.weight [1280 1280]\n",
      "v.blk.27.attn_q.bias [1280]\n",
      "v.blk.27.attn_out.weight [1280 1280]\n",
      "v.blk.27.attn_out.bias [1280]\n",
      "v.blk.27.ln1.weight [1280]\n",
      "v.blk.27.ln1.bias [1280]\n",
      "v.blk.27.ffn_down.weight [1280 5120]\n",
      "v.blk.27.ffn_down.bias [5120]\n",
      "v.blk.27.ffn_up.weight [5120 1280]\n",
      "v.blk.27.ffn_up.bias [1280]\n",
      "v.blk.27.ln2.weight [1280]\n",
      "v.blk.27.ln2.bias [1280]\n",
      "v.blk.28.attn_k.weight [1280 1280]\n",
      "v.blk.28.attn_k.bias [1280]\n",
      "v.blk.28.attn_v.weight [1280 1280]\n",
      "v.blk.28.attn_v.bias [1280]\n",
      "v.blk.28.attn_q.weight [1280 1280]\n",
      "v.blk.28.attn_q.bias [1280]\n",
      "v.blk.28.attn_out.weight [1280 1280]\n",
      "v.blk.28.attn_out.bias [1280]\n",
      "v.blk.28.ln1.weight [1280]\n",
      "v.blk.28.ln1.bias [1280]\n",
      "v.blk.28.ffn_down.weight [1280 5120]\n",
      "v.blk.28.ffn_down.bias [5120]\n",
      "v.blk.28.ffn_up.weight [5120 1280]\n",
      "v.blk.28.ffn_up.bias [1280]\n",
      "v.blk.28.ln2.weight [1280]\n",
      "v.blk.28.ln2.bias [1280]\n",
      "v.blk.29.attn_k.weight [1280 1280]\n",
      "v.blk.29.attn_k.bias [1280]\n",
      "v.blk.29.attn_v.weight [1280 1280]\n",
      "v.blk.29.attn_v.bias [1280]\n",
      "v.blk.29.attn_q.weight [1280 1280]\n",
      "v.blk.29.attn_q.bias [1280]\n",
      "v.blk.29.attn_out.weight [1280 1280]\n",
      "v.blk.29.attn_out.bias [1280]\n",
      "v.blk.29.ln1.weight [1280]\n",
      "v.blk.29.ln1.bias [1280]\n",
      "v.blk.29.ffn_down.weight [1280 5120]\n",
      "v.blk.29.ffn_down.bias [5120]\n",
      "v.blk.29.ffn_up.weight [5120 1280]\n",
      "v.blk.29.ffn_up.bias [1280]\n",
      "v.blk.29.ln2.weight [1280]\n",
      "v.blk.29.ln2.bias [1280]\n",
      "v.blk.30.attn_k.weight [1280 1280]\n",
      "v.blk.30.attn_k.bias [1280]\n",
      "v.blk.30.attn_v.weight [1280 1280]\n",
      "v.blk.30.attn_v.bias [1280]\n",
      "v.blk.30.attn_q.weight [1280 1280]\n",
      "v.blk.30.attn_q.bias [1280]\n",
      "v.blk.30.attn_out.weight [1280 1280]\n",
      "v.blk.30.attn_out.bias [1280]\n",
      "v.blk.30.ln1.weight [1280]\n",
      "v.blk.30.ln1.bias [1280]\n",
      "v.blk.30.ffn_down.weight [1280 5120]\n",
      "v.blk.30.ffn_down.bias [5120]\n",
      "v.blk.30.ffn_up.weight [5120 1280]\n",
      "v.blk.30.ffn_up.bias [1280]\n",
      "v.blk.30.ln2.weight [1280]\n",
      "v.blk.30.ln2.bias [1280]\n"
     ]
    }
   ],
   "source": [
    "import gguf\n",
    "from gguf import gguf_reader\n",
    "\n",
    "# Load the GGUF file\n",
    "model_path = \"/media/m3/models/yi-vl/mmproj-model-f16-q6_k.gguf\"\n",
    "model = gguf_reader.GGUFReader(model_path)\n",
    "\n",
    "for i in range(1000):\n",
    "    try:\n",
    "        print(model.get_tensor(i).name, model.get_tensor(i).shape)\n",
    "    except Exception as e:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mm.1.weight [4608]\n",
      "mm.1.bias [4608]\n",
      "mm.2.weight [4608 5120]\n",
      "mm.2.bias [5120]\n",
      "mm.4.weight [5120 5120]\n",
      "mm.4.bias [5120]\n",
      "v.patch_embd.weight [  14   14    3 1152]\n",
      "v.patch_embd.bias [1152]\n",
      "v.position_embd.weight [1152  729]\n",
      "v.blk.0.attn_k.weight [1152 1152]\n",
      "v.blk.0.attn_k.bias [1152]\n",
      "v.blk.0.attn_v.weight [1152 1152]\n",
      "v.blk.0.attn_v.bias [1152]\n",
      "v.blk.0.attn_q.weight [1152 1152]\n",
      "v.blk.0.attn_q.bias [1152]\n",
      "v.blk.0.attn_out.weight [1152 1152]\n",
      "v.blk.0.attn_out.bias [1152]\n",
      "v.blk.0.ln1.weight [1152]\n",
      "v.blk.0.ln1.bias [1152]\n",
      "v.blk.0.ffn_down.weight [1152 4304]\n",
      "v.blk.0.ffn_down.bias [4304]\n",
      "v.blk.0.ffn_up.weight [4304 1152]\n",
      "v.blk.0.ffn_up.bias [1152]\n",
      "v.blk.0.ln2.weight [1152]\n",
      "v.blk.0.ln2.bias [1152]\n",
      "v.blk.1.attn_k.weight [1152 1152]\n",
      "v.blk.1.attn_k.bias [1152]\n",
      "v.blk.1.attn_v.weight [1152 1152]\n",
      "v.blk.1.attn_v.bias [1152]\n",
      "v.blk.1.attn_q.weight [1152 1152]\n",
      "v.blk.1.attn_q.bias [1152]\n",
      "v.blk.1.attn_out.weight [1152 1152]\n",
      "v.blk.1.attn_out.bias [1152]\n",
      "v.blk.1.ln1.weight [1152]\n",
      "v.blk.1.ln1.bias [1152]\n",
      "v.blk.1.ffn_down.weight [1152 4304]\n",
      "v.blk.1.ffn_down.bias [4304]\n",
      "v.blk.1.ffn_up.weight [4304 1152]\n",
      "v.blk.1.ffn_up.bias [1152]\n",
      "v.blk.1.ln2.weight [1152]\n",
      "v.blk.1.ln2.bias [1152]\n",
      "v.blk.2.attn_k.weight [1152 1152]\n",
      "v.blk.2.attn_k.bias [1152]\n",
      "v.blk.2.attn_v.weight [1152 1152]\n",
      "v.blk.2.attn_v.bias [1152]\n",
      "v.blk.2.attn_q.weight [1152 1152]\n",
      "v.blk.2.attn_q.bias [1152]\n",
      "v.blk.2.attn_out.weight [1152 1152]\n",
      "v.blk.2.attn_out.bias [1152]\n",
      "v.blk.2.ln1.weight [1152]\n",
      "v.blk.2.ln1.bias [1152]\n",
      "v.blk.2.ffn_down.weight [1152 4304]\n",
      "v.blk.2.ffn_down.bias [4304]\n",
      "v.blk.2.ffn_up.weight [4304 1152]\n",
      "v.blk.2.ffn_up.bias [1152]\n",
      "v.blk.2.ln2.weight [1152]\n",
      "v.blk.2.ln2.bias [1152]\n",
      "v.blk.3.attn_k.weight [1152 1152]\n",
      "v.blk.3.attn_k.bias [1152]\n",
      "v.blk.3.attn_v.weight [1152 1152]\n",
      "v.blk.3.attn_v.bias [1152]\n",
      "v.blk.3.attn_q.weight [1152 1152]\n",
      "v.blk.3.attn_q.bias [1152]\n",
      "v.blk.3.attn_out.weight [1152 1152]\n",
      "v.blk.3.attn_out.bias [1152]\n",
      "v.blk.3.ln1.weight [1152]\n",
      "v.blk.3.ln1.bias [1152]\n",
      "v.blk.3.ffn_down.weight [1152 4304]\n",
      "v.blk.3.ffn_down.bias [4304]\n",
      "v.blk.3.ffn_up.weight [4304 1152]\n",
      "v.blk.3.ffn_up.bias [1152]\n",
      "v.blk.3.ln2.weight [1152]\n",
      "v.blk.3.ln2.bias [1152]\n",
      "v.blk.4.attn_k.weight [1152 1152]\n",
      "v.blk.4.attn_k.bias [1152]\n",
      "v.blk.4.attn_v.weight [1152 1152]\n",
      "v.blk.4.attn_v.bias [1152]\n",
      "v.blk.4.attn_q.weight [1152 1152]\n",
      "v.blk.4.attn_q.bias [1152]\n",
      "v.blk.4.attn_out.weight [1152 1152]\n",
      "v.blk.4.attn_out.bias [1152]\n",
      "v.blk.4.ln1.weight [1152]\n",
      "v.blk.4.ln1.bias [1152]\n",
      "v.blk.4.ffn_down.weight [1152 4304]\n",
      "v.blk.4.ffn_down.bias [4304]\n",
      "v.blk.4.ffn_up.weight [4304 1152]\n",
      "v.blk.4.ffn_up.bias [1152]\n",
      "v.blk.4.ln2.weight [1152]\n",
      "v.blk.4.ln2.bias [1152]\n",
      "v.blk.5.attn_k.weight [1152 1152]\n",
      "v.blk.5.attn_k.bias [1152]\n",
      "v.blk.5.attn_v.weight [1152 1152]\n",
      "v.blk.5.attn_v.bias [1152]\n",
      "v.blk.5.attn_q.weight [1152 1152]\n",
      "v.blk.5.attn_q.bias [1152]\n",
      "v.blk.5.attn_out.weight [1152 1152]\n",
      "v.blk.5.attn_out.bias [1152]\n",
      "v.blk.5.ln1.weight [1152]\n",
      "v.blk.5.ln1.bias [1152]\n",
      "v.blk.5.ffn_down.weight [1152 4304]\n",
      "v.blk.5.ffn_down.bias [4304]\n",
      "v.blk.5.ffn_up.weight [4304 1152]\n",
      "v.blk.5.ffn_up.bias [1152]\n",
      "v.blk.5.ln2.weight [1152]\n",
      "v.blk.5.ln2.bias [1152]\n",
      "v.blk.6.attn_k.weight [1152 1152]\n",
      "v.blk.6.attn_k.bias [1152]\n",
      "v.blk.6.attn_v.weight [1152 1152]\n",
      "v.blk.6.attn_v.bias [1152]\n",
      "v.blk.6.attn_q.weight [1152 1152]\n",
      "v.blk.6.attn_q.bias [1152]\n",
      "v.blk.6.attn_out.weight [1152 1152]\n",
      "v.blk.6.attn_out.bias [1152]\n",
      "v.blk.6.ln1.weight [1152]\n",
      "v.blk.6.ln1.bias [1152]\n",
      "v.blk.6.ffn_down.weight [1152 4304]\n",
      "v.blk.6.ffn_down.bias [4304]\n",
      "v.blk.6.ffn_up.weight [4304 1152]\n",
      "v.blk.6.ffn_up.bias [1152]\n",
      "v.blk.6.ln2.weight [1152]\n",
      "v.blk.6.ln2.bias [1152]\n",
      "v.blk.7.attn_k.weight [1152 1152]\n",
      "v.blk.7.attn_k.bias [1152]\n",
      "v.blk.7.attn_v.weight [1152 1152]\n",
      "v.blk.7.attn_v.bias [1152]\n",
      "v.blk.7.attn_q.weight [1152 1152]\n",
      "v.blk.7.attn_q.bias [1152]\n",
      "v.blk.7.attn_out.weight [1152 1152]\n",
      "v.blk.7.attn_out.bias [1152]\n",
      "v.blk.7.ln1.weight [1152]\n",
      "v.blk.7.ln1.bias [1152]\n",
      "v.blk.7.ffn_down.weight [1152 4304]\n",
      "v.blk.7.ffn_down.bias [4304]\n",
      "v.blk.7.ffn_up.weight [4304 1152]\n",
      "v.blk.7.ffn_up.bias [1152]\n",
      "v.blk.7.ln2.weight [1152]\n",
      "v.blk.7.ln2.bias [1152]\n",
      "v.blk.8.attn_k.weight [1152 1152]\n",
      "v.blk.8.attn_k.bias [1152]\n",
      "v.blk.8.attn_v.weight [1152 1152]\n",
      "v.blk.8.attn_v.bias [1152]\n",
      "v.blk.8.attn_q.weight [1152 1152]\n",
      "v.blk.8.attn_q.bias [1152]\n",
      "v.blk.8.attn_out.weight [1152 1152]\n",
      "v.blk.8.attn_out.bias [1152]\n",
      "v.blk.8.ln1.weight [1152]\n",
      "v.blk.8.ln1.bias [1152]\n",
      "v.blk.8.ffn_down.weight [1152 4304]\n",
      "v.blk.8.ffn_down.bias [4304]\n",
      "v.blk.8.ffn_up.weight [4304 1152]\n",
      "v.blk.8.ffn_up.bias [1152]\n",
      "v.blk.8.ln2.weight [1152]\n",
      "v.blk.8.ln2.bias [1152]\n",
      "v.blk.9.attn_k.weight [1152 1152]\n",
      "v.blk.9.attn_k.bias [1152]\n",
      "v.blk.9.attn_v.weight [1152 1152]\n",
      "v.blk.9.attn_v.bias [1152]\n",
      "v.blk.9.attn_q.weight [1152 1152]\n",
      "v.blk.9.attn_q.bias [1152]\n",
      "v.blk.9.attn_out.weight [1152 1152]\n",
      "v.blk.9.attn_out.bias [1152]\n",
      "v.blk.9.ln1.weight [1152]\n",
      "v.blk.9.ln1.bias [1152]\n",
      "v.blk.9.ffn_down.weight [1152 4304]\n",
      "v.blk.9.ffn_down.bias [4304]\n",
      "v.blk.9.ffn_up.weight [4304 1152]\n",
      "v.blk.9.ffn_up.bias [1152]\n",
      "v.blk.9.ln2.weight [1152]\n",
      "v.blk.9.ln2.bias [1152]\n",
      "v.blk.10.attn_k.weight [1152 1152]\n",
      "v.blk.10.attn_k.bias [1152]\n",
      "v.blk.10.attn_v.weight [1152 1152]\n",
      "v.blk.10.attn_v.bias [1152]\n",
      "v.blk.10.attn_q.weight [1152 1152]\n",
      "v.blk.10.attn_q.bias [1152]\n",
      "v.blk.10.attn_out.weight [1152 1152]\n",
      "v.blk.10.attn_out.bias [1152]\n",
      "v.blk.10.ln1.weight [1152]\n",
      "v.blk.10.ln1.bias [1152]\n",
      "v.blk.10.ffn_down.weight [1152 4304]\n",
      "v.blk.10.ffn_down.bias [4304]\n",
      "v.blk.10.ffn_up.weight [4304 1152]\n",
      "v.blk.10.ffn_up.bias [1152]\n",
      "v.blk.10.ln2.weight [1152]\n",
      "v.blk.10.ln2.bias [1152]\n",
      "v.blk.11.attn_k.weight [1152 1152]\n",
      "v.blk.11.attn_k.bias [1152]\n",
      "v.blk.11.attn_v.weight [1152 1152]\n",
      "v.blk.11.attn_v.bias [1152]\n",
      "v.blk.11.attn_q.weight [1152 1152]\n",
      "v.blk.11.attn_q.bias [1152]\n",
      "v.blk.11.attn_out.weight [1152 1152]\n",
      "v.blk.11.attn_out.bias [1152]\n",
      "v.blk.11.ln1.weight [1152]\n",
      "v.blk.11.ln1.bias [1152]\n",
      "v.blk.11.ffn_down.weight [1152 4304]\n",
      "v.blk.11.ffn_down.bias [4304]\n",
      "v.blk.11.ffn_up.weight [4304 1152]\n",
      "v.blk.11.ffn_up.bias [1152]\n",
      "v.blk.11.ln2.weight [1152]\n",
      "v.blk.11.ln2.bias [1152]\n",
      "v.blk.12.attn_k.weight [1152 1152]\n",
      "v.blk.12.attn_k.bias [1152]\n",
      "v.blk.12.attn_v.weight [1152 1152]\n",
      "v.blk.12.attn_v.bias [1152]\n",
      "v.blk.12.attn_q.weight [1152 1152]\n",
      "v.blk.12.attn_q.bias [1152]\n",
      "v.blk.12.attn_out.weight [1152 1152]\n",
      "v.blk.12.attn_out.bias [1152]\n",
      "v.blk.12.ln1.weight [1152]\n",
      "v.blk.12.ln1.bias [1152]\n",
      "v.blk.12.ffn_down.weight [1152 4304]\n",
      "v.blk.12.ffn_down.bias [4304]\n",
      "v.blk.12.ffn_up.weight [4304 1152]\n",
      "v.blk.12.ffn_up.bias [1152]\n",
      "v.blk.12.ln2.weight [1152]\n",
      "v.blk.12.ln2.bias [1152]\n",
      "v.blk.13.attn_k.weight [1152 1152]\n",
      "v.blk.13.attn_k.bias [1152]\n",
      "v.blk.13.attn_v.weight [1152 1152]\n",
      "v.blk.13.attn_v.bias [1152]\n",
      "v.blk.13.attn_q.weight [1152 1152]\n",
      "v.blk.13.attn_q.bias [1152]\n",
      "v.blk.13.attn_out.weight [1152 1152]\n",
      "v.blk.13.attn_out.bias [1152]\n",
      "v.blk.13.ln1.weight [1152]\n",
      "v.blk.13.ln1.bias [1152]\n",
      "v.blk.13.ffn_down.weight [1152 4304]\n",
      "v.blk.13.ffn_down.bias [4304]\n",
      "v.blk.13.ffn_up.weight [4304 1152]\n",
      "v.blk.13.ffn_up.bias [1152]\n",
      "v.blk.13.ln2.weight [1152]\n",
      "v.blk.13.ln2.bias [1152]\n",
      "v.blk.14.attn_k.weight [1152 1152]\n",
      "v.blk.14.attn_k.bias [1152]\n",
      "v.blk.14.attn_v.weight [1152 1152]\n",
      "v.blk.14.attn_v.bias [1152]\n",
      "v.blk.14.attn_q.weight [1152 1152]\n",
      "v.blk.14.attn_q.bias [1152]\n",
      "v.blk.14.attn_out.weight [1152 1152]\n",
      "v.blk.14.attn_out.bias [1152]\n",
      "v.blk.14.ln1.weight [1152]\n",
      "v.blk.14.ln1.bias [1152]\n",
      "v.blk.14.ffn_down.weight [1152 4304]\n",
      "v.blk.14.ffn_down.bias [4304]\n",
      "v.blk.14.ffn_up.weight [4304 1152]\n",
      "v.blk.14.ffn_up.bias [1152]\n",
      "v.blk.14.ln2.weight [1152]\n",
      "v.blk.14.ln2.bias [1152]\n",
      "v.blk.15.attn_k.weight [1152 1152]\n",
      "v.blk.15.attn_k.bias [1152]\n",
      "v.blk.15.attn_v.weight [1152 1152]\n",
      "v.blk.15.attn_v.bias [1152]\n",
      "v.blk.15.attn_q.weight [1152 1152]\n",
      "v.blk.15.attn_q.bias [1152]\n",
      "v.blk.15.attn_out.weight [1152 1152]\n",
      "v.blk.15.attn_out.bias [1152]\n",
      "v.blk.15.ln1.weight [1152]\n",
      "v.blk.15.ln1.bias [1152]\n",
      "v.blk.15.ffn_down.weight [1152 4304]\n",
      "v.blk.15.ffn_down.bias [4304]\n",
      "v.blk.15.ffn_up.weight [4304 1152]\n",
      "v.blk.15.ffn_up.bias [1152]\n",
      "v.blk.15.ln2.weight [1152]\n",
      "v.blk.15.ln2.bias [1152]\n",
      "v.blk.16.attn_k.weight [1152 1152]\n",
      "v.blk.16.attn_k.bias [1152]\n",
      "v.blk.16.attn_v.weight [1152 1152]\n",
      "v.blk.16.attn_v.bias [1152]\n",
      "v.blk.16.attn_q.weight [1152 1152]\n",
      "v.blk.16.attn_q.bias [1152]\n",
      "v.blk.16.attn_out.weight [1152 1152]\n",
      "v.blk.16.attn_out.bias [1152]\n",
      "v.blk.16.ln1.weight [1152]\n",
      "v.blk.16.ln1.bias [1152]\n",
      "v.blk.16.ffn_down.weight [1152 4304]\n",
      "v.blk.16.ffn_down.bias [4304]\n",
      "v.blk.16.ffn_up.weight [4304 1152]\n",
      "v.blk.16.ffn_up.bias [1152]\n",
      "v.blk.16.ln2.weight [1152]\n",
      "v.blk.16.ln2.bias [1152]\n",
      "v.blk.17.attn_k.weight [1152 1152]\n",
      "v.blk.17.attn_k.bias [1152]\n",
      "v.blk.17.attn_v.weight [1152 1152]\n",
      "v.blk.17.attn_v.bias [1152]\n",
      "v.blk.17.attn_q.weight [1152 1152]\n",
      "v.blk.17.attn_q.bias [1152]\n",
      "v.blk.17.attn_out.weight [1152 1152]\n",
      "v.blk.17.attn_out.bias [1152]\n",
      "v.blk.17.ln1.weight [1152]\n",
      "v.blk.17.ln1.bias [1152]\n",
      "v.blk.17.ffn_down.weight [1152 4304]\n",
      "v.blk.17.ffn_down.bias [4304]\n",
      "v.blk.17.ffn_up.weight [4304 1152]\n",
      "v.blk.17.ffn_up.bias [1152]\n",
      "v.blk.17.ln2.weight [1152]\n",
      "v.blk.17.ln2.bias [1152]\n",
      "v.blk.18.attn_k.weight [1152 1152]\n",
      "v.blk.18.attn_k.bias [1152]\n",
      "v.blk.18.attn_v.weight [1152 1152]\n",
      "v.blk.18.attn_v.bias [1152]\n",
      "v.blk.18.attn_q.weight [1152 1152]\n",
      "v.blk.18.attn_q.bias [1152]\n",
      "v.blk.18.attn_out.weight [1152 1152]\n",
      "v.blk.18.attn_out.bias [1152]\n",
      "v.blk.18.ln1.weight [1152]\n",
      "v.blk.18.ln1.bias [1152]\n",
      "v.blk.18.ffn_down.weight [1152 4304]\n",
      "v.blk.18.ffn_down.bias [4304]\n",
      "v.blk.18.ffn_up.weight [4304 1152]\n",
      "v.blk.18.ffn_up.bias [1152]\n",
      "v.blk.18.ln2.weight [1152]\n",
      "v.blk.18.ln2.bias [1152]\n",
      "v.blk.19.attn_k.weight [1152 1152]\n",
      "v.blk.19.attn_k.bias [1152]\n",
      "v.blk.19.attn_v.weight [1152 1152]\n",
      "v.blk.19.attn_v.bias [1152]\n",
      "v.blk.19.attn_q.weight [1152 1152]\n",
      "v.blk.19.attn_q.bias [1152]\n",
      "v.blk.19.attn_out.weight [1152 1152]\n",
      "v.blk.19.attn_out.bias [1152]\n",
      "v.blk.19.ln1.weight [1152]\n",
      "v.blk.19.ln1.bias [1152]\n",
      "v.blk.19.ffn_down.weight [1152 4304]\n",
      "v.blk.19.ffn_down.bias [4304]\n",
      "v.blk.19.ffn_up.weight [4304 1152]\n",
      "v.blk.19.ffn_up.bias [1152]\n",
      "v.blk.19.ln2.weight [1152]\n",
      "v.blk.19.ln2.bias [1152]\n",
      "v.blk.20.attn_k.weight [1152 1152]\n",
      "v.blk.20.attn_k.bias [1152]\n",
      "v.blk.20.attn_v.weight [1152 1152]\n",
      "v.blk.20.attn_v.bias [1152]\n",
      "v.blk.20.attn_q.weight [1152 1152]\n",
      "v.blk.20.attn_q.bias [1152]\n",
      "v.blk.20.attn_out.weight [1152 1152]\n",
      "v.blk.20.attn_out.bias [1152]\n",
      "v.blk.20.ln1.weight [1152]\n",
      "v.blk.20.ln1.bias [1152]\n",
      "v.blk.20.ffn_down.weight [1152 4304]\n",
      "v.blk.20.ffn_down.bias [4304]\n",
      "v.blk.20.ffn_up.weight [4304 1152]\n",
      "v.blk.20.ffn_up.bias [1152]\n",
      "v.blk.20.ln2.weight [1152]\n",
      "v.blk.20.ln2.bias [1152]\n",
      "v.blk.21.attn_k.weight [1152 1152]\n",
      "v.blk.21.attn_k.bias [1152]\n",
      "v.blk.21.attn_v.weight [1152 1152]\n",
      "v.blk.21.attn_v.bias [1152]\n",
      "v.blk.21.attn_q.weight [1152 1152]\n",
      "v.blk.21.attn_q.bias [1152]\n",
      "v.blk.21.attn_out.weight [1152 1152]\n",
      "v.blk.21.attn_out.bias [1152]\n",
      "v.blk.21.ln1.weight [1152]\n",
      "v.blk.21.ln1.bias [1152]\n",
      "v.blk.21.ffn_down.weight [1152 4304]\n",
      "v.blk.21.ffn_down.bias [4304]\n",
      "v.blk.21.ffn_up.weight [4304 1152]\n",
      "v.blk.21.ffn_up.bias [1152]\n",
      "v.blk.21.ln2.weight [1152]\n",
      "v.blk.21.ln2.bias [1152]\n",
      "v.blk.22.attn_k.weight [1152 1152]\n",
      "v.blk.22.attn_k.bias [1152]\n",
      "v.blk.22.attn_v.weight [1152 1152]\n",
      "v.blk.22.attn_v.bias [1152]\n",
      "v.blk.22.attn_q.weight [1152 1152]\n",
      "v.blk.22.attn_q.bias [1152]\n",
      "v.blk.22.attn_out.weight [1152 1152]\n",
      "v.blk.22.attn_out.bias [1152]\n",
      "v.blk.22.ln1.weight [1152]\n",
      "v.blk.22.ln1.bias [1152]\n",
      "v.blk.22.ffn_down.weight [1152 4304]\n",
      "v.blk.22.ffn_down.bias [4304]\n",
      "v.blk.22.ffn_up.weight [4304 1152]\n",
      "v.blk.22.ffn_up.bias [1152]\n",
      "v.blk.22.ln2.weight [1152]\n",
      "v.blk.22.ln2.bias [1152]\n",
      "v.blk.23.attn_k.weight [1152 1152]\n",
      "v.blk.23.attn_k.bias [1152]\n",
      "v.blk.23.attn_v.weight [1152 1152]\n",
      "v.blk.23.attn_v.bias [1152]\n",
      "v.blk.23.attn_q.weight [1152 1152]\n",
      "v.blk.23.attn_q.bias [1152]\n",
      "v.blk.23.attn_out.weight [1152 1152]\n",
      "v.blk.23.attn_out.bias [1152]\n",
      "v.blk.23.ln1.weight [1152]\n",
      "v.blk.23.ln1.bias [1152]\n",
      "v.blk.23.ffn_down.weight [1152 4304]\n",
      "v.blk.23.ffn_down.bias [4304]\n",
      "v.blk.23.ffn_up.weight [4304 1152]\n",
      "v.blk.23.ffn_up.bias [1152]\n",
      "v.blk.23.ln2.weight [1152]\n",
      "v.blk.23.ln2.bias [1152]\n",
      "v.blk.24.attn_k.weight [1152 1152]\n",
      "v.blk.24.attn_k.bias [1152]\n",
      "v.blk.24.attn_v.weight [1152 1152]\n",
      "v.blk.24.attn_v.bias [1152]\n",
      "v.blk.24.attn_q.weight [1152 1152]\n",
      "v.blk.24.attn_q.bias [1152]\n",
      "v.blk.24.attn_out.weight [1152 1152]\n",
      "v.blk.24.attn_out.bias [1152]\n",
      "v.blk.24.ln1.weight [1152]\n",
      "v.blk.24.ln1.bias [1152]\n",
      "v.blk.24.ffn_down.weight [1152 4304]\n",
      "v.blk.24.ffn_down.bias [4304]\n",
      "v.blk.24.ffn_up.weight [4304 1152]\n",
      "v.blk.24.ffn_up.bias [1152]\n",
      "v.blk.24.ln2.weight [1152]\n",
      "v.blk.24.ln2.bias [1152]\n",
      "v.blk.25.attn_k.weight [1152 1152]\n",
      "v.blk.25.attn_k.bias [1152]\n",
      "v.blk.25.attn_v.weight [1152 1152]\n",
      "v.blk.25.attn_v.bias [1152]\n",
      "v.blk.25.attn_q.weight [1152 1152]\n",
      "v.blk.25.attn_q.bias [1152]\n",
      "v.blk.25.attn_out.weight [1152 1152]\n",
      "v.blk.25.attn_out.bias [1152]\n",
      "v.blk.25.ln1.weight [1152]\n",
      "v.blk.25.ln1.bias [1152]\n",
      "v.blk.25.ffn_down.weight [1152 4304]\n",
      "v.blk.25.ffn_down.bias [4304]\n",
      "v.blk.25.ffn_up.weight [4304 1152]\n",
      "v.blk.25.ffn_up.bias [1152]\n",
      "v.blk.25.ln2.weight [1152]\n",
      "v.blk.25.ln2.bias [1152]\n",
      "v.blk.26.attn_k.weight [1152 1152]\n",
      "v.blk.26.attn_k.bias [1152]\n",
      "v.blk.26.attn_v.weight [1152 1152]\n",
      "v.blk.26.attn_v.bias [1152]\n",
      "v.blk.26.attn_q.weight [1152 1152]\n",
      "v.blk.26.attn_q.bias [1152]\n",
      "v.blk.26.attn_out.weight [1152 1152]\n",
      "v.blk.26.attn_out.bias [1152]\n",
      "v.blk.26.ln1.weight [1152]\n",
      "v.blk.26.ln1.bias [1152]\n",
      "v.blk.26.ffn_down.weight [1152 4304]\n",
      "v.blk.26.ffn_down.bias [4304]\n",
      "v.blk.26.ffn_up.weight [4304 1152]\n",
      "v.blk.26.ffn_up.bias [1152]\n",
      "v.blk.26.ln2.weight [1152]\n",
      "v.blk.26.ln2.bias [1152]\n",
      "v.post_ln.weight [1152]\n",
      "v.post_ln.bias [1152]\n"
     ]
    }
   ],
   "source": [
    "import gguf\n",
    "from gguf import gguf_reader\n",
    "\n",
    "# Load the GGUF file\n",
    "model_path = \"/media/m3/models/CholecT50_vila_13B_3_Epoch/mmproj-model-f16.gguf\"\n",
    "model = gguf_reader.GGUFReader(model_path)\n",
    "\n",
    "for i in range(1000):\n",
    "    try:\n",
    "        print(model.get_tensor(i).name, model.get_tensor(i).shape)\n",
    "    except Exception as e:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resampler.query [3584   64]\n",
      "resampler.pos_embed_k [3584 4900]\n",
      "resampler.proj.weight [3584 3584]\n",
      "resampler.kv.weight [1152 3584]\n",
      "resampler.attn.q.weight [3584 3584]\n",
      "resampler.attn.k.weight [3584 3584]\n",
      "resampler.attn.v.weight [3584 3584]\n",
      "resampler.attn.q.bias [3584]\n",
      "resampler.attn.k.bias [3584]\n",
      "resampler.attn.v.bias [3584]\n",
      "resampler.attn.out.weight [3584 3584]\n",
      "resampler.attn.out.bias [3584]\n",
      "resampler.ln_q.weight [3584]\n",
      "resampler.ln_q.bias [3584]\n",
      "resampler.ln_kv.weight [3584]\n",
      "resampler.ln_kv.bias [3584]\n",
      "resampler.ln_post.weight [3584]\n",
      "resampler.ln_post.bias [3584]\n",
      "v.patch_embd.weight [  14   14    3 1152]\n",
      "v.patch_embd.bias [1152]\n",
      "v.position_embd.weight [1152 4900]\n",
      "v.blk.0.attn_k.weight [1152 1152]\n",
      "v.blk.0.attn_k.bias [1152]\n",
      "v.blk.0.attn_v.weight [1152 1152]\n",
      "v.blk.0.attn_v.bias [1152]\n",
      "v.blk.0.attn_q.weight [1152 1152]\n",
      "v.blk.0.attn_q.bias [1152]\n",
      "v.blk.0.attn_out.weight [1152 1152]\n",
      "v.blk.0.attn_out.bias [1152]\n",
      "v.blk.0.ln1.weight [1152]\n",
      "v.blk.0.ln1.bias [1152]\n",
      "v.blk.0.ffn_down.weight [1152 4304]\n",
      "v.blk.0.ffn_down.bias [4304]\n",
      "v.blk.0.ffn_up.weight [4304 1152]\n",
      "v.blk.0.ffn_up.bias [1152]\n",
      "v.blk.0.ln2.weight [1152]\n",
      "v.blk.0.ln2.bias [1152]\n",
      "v.blk.1.attn_k.weight [1152 1152]\n",
      "v.blk.1.attn_k.bias [1152]\n",
      "v.blk.1.attn_v.weight [1152 1152]\n",
      "v.blk.1.attn_v.bias [1152]\n",
      "v.blk.1.attn_q.weight [1152 1152]\n",
      "v.blk.1.attn_q.bias [1152]\n",
      "v.blk.1.attn_out.weight [1152 1152]\n",
      "v.blk.1.attn_out.bias [1152]\n",
      "v.blk.1.ln1.weight [1152]\n",
      "v.blk.1.ln1.bias [1152]\n",
      "v.blk.1.ffn_down.weight [1152 4304]\n",
      "v.blk.1.ffn_down.bias [4304]\n",
      "v.blk.1.ffn_up.weight [4304 1152]\n",
      "v.blk.1.ffn_up.bias [1152]\n",
      "v.blk.1.ln2.weight [1152]\n",
      "v.blk.1.ln2.bias [1152]\n",
      "v.blk.2.attn_k.weight [1152 1152]\n",
      "v.blk.2.attn_k.bias [1152]\n",
      "v.blk.2.attn_v.weight [1152 1152]\n",
      "v.blk.2.attn_v.bias [1152]\n",
      "v.blk.2.attn_q.weight [1152 1152]\n",
      "v.blk.2.attn_q.bias [1152]\n",
      "v.blk.2.attn_out.weight [1152 1152]\n",
      "v.blk.2.attn_out.bias [1152]\n",
      "v.blk.2.ln1.weight [1152]\n",
      "v.blk.2.ln1.bias [1152]\n",
      "v.blk.2.ffn_down.weight [1152 4304]\n",
      "v.blk.2.ffn_down.bias [4304]\n",
      "v.blk.2.ffn_up.weight [4304 1152]\n",
      "v.blk.2.ffn_up.bias [1152]\n",
      "v.blk.2.ln2.weight [1152]\n",
      "v.blk.2.ln2.bias [1152]\n",
      "v.blk.3.attn_k.weight [1152 1152]\n",
      "v.blk.3.attn_k.bias [1152]\n",
      "v.blk.3.attn_v.weight [1152 1152]\n",
      "v.blk.3.attn_v.bias [1152]\n",
      "v.blk.3.attn_q.weight [1152 1152]\n",
      "v.blk.3.attn_q.bias [1152]\n",
      "v.blk.3.attn_out.weight [1152 1152]\n",
      "v.blk.3.attn_out.bias [1152]\n",
      "v.blk.3.ln1.weight [1152]\n",
      "v.blk.3.ln1.bias [1152]\n",
      "v.blk.3.ffn_down.weight [1152 4304]\n",
      "v.blk.3.ffn_down.bias [4304]\n",
      "v.blk.3.ffn_up.weight [4304 1152]\n",
      "v.blk.3.ffn_up.bias [1152]\n",
      "v.blk.3.ln2.weight [1152]\n",
      "v.blk.3.ln2.bias [1152]\n",
      "v.blk.4.attn_k.weight [1152 1152]\n",
      "v.blk.4.attn_k.bias [1152]\n",
      "v.blk.4.attn_v.weight [1152 1152]\n",
      "v.blk.4.attn_v.bias [1152]\n",
      "v.blk.4.attn_q.weight [1152 1152]\n",
      "v.blk.4.attn_q.bias [1152]\n",
      "v.blk.4.attn_out.weight [1152 1152]\n",
      "v.blk.4.attn_out.bias [1152]\n",
      "v.blk.4.ln1.weight [1152]\n",
      "v.blk.4.ln1.bias [1152]\n",
      "v.blk.4.ffn_down.weight [1152 4304]\n",
      "v.blk.4.ffn_down.bias [4304]\n",
      "v.blk.4.ffn_up.weight [4304 1152]\n",
      "v.blk.4.ffn_up.bias [1152]\n",
      "v.blk.4.ln2.weight [1152]\n",
      "v.blk.4.ln2.bias [1152]\n",
      "v.blk.5.attn_k.weight [1152 1152]\n",
      "v.blk.5.attn_k.bias [1152]\n",
      "v.blk.5.attn_v.weight [1152 1152]\n",
      "v.blk.5.attn_v.bias [1152]\n",
      "v.blk.5.attn_q.weight [1152 1152]\n",
      "v.blk.5.attn_q.bias [1152]\n",
      "v.blk.5.attn_out.weight [1152 1152]\n",
      "v.blk.5.attn_out.bias [1152]\n",
      "v.blk.5.ln1.weight [1152]\n",
      "v.blk.5.ln1.bias [1152]\n",
      "v.blk.5.ffn_down.weight [1152 4304]\n",
      "v.blk.5.ffn_down.bias [4304]\n",
      "v.blk.5.ffn_up.weight [4304 1152]\n",
      "v.blk.5.ffn_up.bias [1152]\n",
      "v.blk.5.ln2.weight [1152]\n",
      "v.blk.5.ln2.bias [1152]\n",
      "v.blk.6.attn_k.weight [1152 1152]\n",
      "v.blk.6.attn_k.bias [1152]\n",
      "v.blk.6.attn_v.weight [1152 1152]\n",
      "v.blk.6.attn_v.bias [1152]\n",
      "v.blk.6.attn_q.weight [1152 1152]\n",
      "v.blk.6.attn_q.bias [1152]\n",
      "v.blk.6.attn_out.weight [1152 1152]\n",
      "v.blk.6.attn_out.bias [1152]\n",
      "v.blk.6.ln1.weight [1152]\n",
      "v.blk.6.ln1.bias [1152]\n",
      "v.blk.6.ffn_down.weight [1152 4304]\n",
      "v.blk.6.ffn_down.bias [4304]\n",
      "v.blk.6.ffn_up.weight [4304 1152]\n",
      "v.blk.6.ffn_up.bias [1152]\n",
      "v.blk.6.ln2.weight [1152]\n",
      "v.blk.6.ln2.bias [1152]\n",
      "v.blk.7.attn_k.weight [1152 1152]\n",
      "v.blk.7.attn_k.bias [1152]\n",
      "v.blk.7.attn_v.weight [1152 1152]\n",
      "v.blk.7.attn_v.bias [1152]\n",
      "v.blk.7.attn_q.weight [1152 1152]\n",
      "v.blk.7.attn_q.bias [1152]\n",
      "v.blk.7.attn_out.weight [1152 1152]\n",
      "v.blk.7.attn_out.bias [1152]\n",
      "v.blk.7.ln1.weight [1152]\n",
      "v.blk.7.ln1.bias [1152]\n",
      "v.blk.7.ffn_down.weight [1152 4304]\n",
      "v.blk.7.ffn_down.bias [4304]\n",
      "v.blk.7.ffn_up.weight [4304 1152]\n",
      "v.blk.7.ffn_up.bias [1152]\n",
      "v.blk.7.ln2.weight [1152]\n",
      "v.blk.7.ln2.bias [1152]\n",
      "v.blk.8.attn_k.weight [1152 1152]\n",
      "v.blk.8.attn_k.bias [1152]\n",
      "v.blk.8.attn_v.weight [1152 1152]\n",
      "v.blk.8.attn_v.bias [1152]\n",
      "v.blk.8.attn_q.weight [1152 1152]\n",
      "v.blk.8.attn_q.bias [1152]\n",
      "v.blk.8.attn_out.weight [1152 1152]\n",
      "v.blk.8.attn_out.bias [1152]\n",
      "v.blk.8.ln1.weight [1152]\n",
      "v.blk.8.ln1.bias [1152]\n",
      "v.blk.8.ffn_down.weight [1152 4304]\n",
      "v.blk.8.ffn_down.bias [4304]\n",
      "v.blk.8.ffn_up.weight [4304 1152]\n",
      "v.blk.8.ffn_up.bias [1152]\n",
      "v.blk.8.ln2.weight [1152]\n",
      "v.blk.8.ln2.bias [1152]\n",
      "v.blk.9.attn_k.weight [1152 1152]\n",
      "v.blk.9.attn_k.bias [1152]\n",
      "v.blk.9.attn_v.weight [1152 1152]\n",
      "v.blk.9.attn_v.bias [1152]\n",
      "v.blk.9.attn_q.weight [1152 1152]\n",
      "v.blk.9.attn_q.bias [1152]\n",
      "v.blk.9.attn_out.weight [1152 1152]\n",
      "v.blk.9.attn_out.bias [1152]\n",
      "v.blk.9.ln1.weight [1152]\n",
      "v.blk.9.ln1.bias [1152]\n",
      "v.blk.9.ffn_down.weight [1152 4304]\n",
      "v.blk.9.ffn_down.bias [4304]\n",
      "v.blk.9.ffn_up.weight [4304 1152]\n",
      "v.blk.9.ffn_up.bias [1152]\n",
      "v.blk.9.ln2.weight [1152]\n",
      "v.blk.9.ln2.bias [1152]\n",
      "v.blk.10.attn_k.weight [1152 1152]\n",
      "v.blk.10.attn_k.bias [1152]\n",
      "v.blk.10.attn_v.weight [1152 1152]\n",
      "v.blk.10.attn_v.bias [1152]\n",
      "v.blk.10.attn_q.weight [1152 1152]\n",
      "v.blk.10.attn_q.bias [1152]\n",
      "v.blk.10.attn_out.weight [1152 1152]\n",
      "v.blk.10.attn_out.bias [1152]\n",
      "v.blk.10.ln1.weight [1152]\n",
      "v.blk.10.ln1.bias [1152]\n",
      "v.blk.10.ffn_down.weight [1152 4304]\n",
      "v.blk.10.ffn_down.bias [4304]\n",
      "v.blk.10.ffn_up.weight [4304 1152]\n",
      "v.blk.10.ffn_up.bias [1152]\n",
      "v.blk.10.ln2.weight [1152]\n",
      "v.blk.10.ln2.bias [1152]\n",
      "v.blk.11.attn_k.weight [1152 1152]\n",
      "v.blk.11.attn_k.bias [1152]\n",
      "v.blk.11.attn_v.weight [1152 1152]\n",
      "v.blk.11.attn_v.bias [1152]\n",
      "v.blk.11.attn_q.weight [1152 1152]\n",
      "v.blk.11.attn_q.bias [1152]\n",
      "v.blk.11.attn_out.weight [1152 1152]\n",
      "v.blk.11.attn_out.bias [1152]\n",
      "v.blk.11.ln1.weight [1152]\n",
      "v.blk.11.ln1.bias [1152]\n",
      "v.blk.11.ffn_down.weight [1152 4304]\n",
      "v.blk.11.ffn_down.bias [4304]\n",
      "v.blk.11.ffn_up.weight [4304 1152]\n",
      "v.blk.11.ffn_up.bias [1152]\n",
      "v.blk.11.ln2.weight [1152]\n",
      "v.blk.11.ln2.bias [1152]\n",
      "v.blk.12.attn_k.weight [1152 1152]\n",
      "v.blk.12.attn_k.bias [1152]\n",
      "v.blk.12.attn_v.weight [1152 1152]\n",
      "v.blk.12.attn_v.bias [1152]\n",
      "v.blk.12.attn_q.weight [1152 1152]\n",
      "v.blk.12.attn_q.bias [1152]\n",
      "v.blk.12.attn_out.weight [1152 1152]\n",
      "v.blk.12.attn_out.bias [1152]\n",
      "v.blk.12.ln1.weight [1152]\n",
      "v.blk.12.ln1.bias [1152]\n",
      "v.blk.12.ffn_down.weight [1152 4304]\n",
      "v.blk.12.ffn_down.bias [4304]\n",
      "v.blk.12.ffn_up.weight [4304 1152]\n",
      "v.blk.12.ffn_up.bias [1152]\n",
      "v.blk.12.ln2.weight [1152]\n",
      "v.blk.12.ln2.bias [1152]\n",
      "v.blk.13.attn_k.weight [1152 1152]\n",
      "v.blk.13.attn_k.bias [1152]\n",
      "v.blk.13.attn_v.weight [1152 1152]\n",
      "v.blk.13.attn_v.bias [1152]\n",
      "v.blk.13.attn_q.weight [1152 1152]\n",
      "v.blk.13.attn_q.bias [1152]\n",
      "v.blk.13.attn_out.weight [1152 1152]\n",
      "v.blk.13.attn_out.bias [1152]\n",
      "v.blk.13.ln1.weight [1152]\n",
      "v.blk.13.ln1.bias [1152]\n",
      "v.blk.13.ffn_down.weight [1152 4304]\n",
      "v.blk.13.ffn_down.bias [4304]\n",
      "v.blk.13.ffn_up.weight [4304 1152]\n",
      "v.blk.13.ffn_up.bias [1152]\n",
      "v.blk.13.ln2.weight [1152]\n",
      "v.blk.13.ln2.bias [1152]\n",
      "v.blk.14.attn_k.weight [1152 1152]\n",
      "v.blk.14.attn_k.bias [1152]\n",
      "v.blk.14.attn_v.weight [1152 1152]\n",
      "v.blk.14.attn_v.bias [1152]\n",
      "v.blk.14.attn_q.weight [1152 1152]\n",
      "v.blk.14.attn_q.bias [1152]\n",
      "v.blk.14.attn_out.weight [1152 1152]\n",
      "v.blk.14.attn_out.bias [1152]\n",
      "v.blk.14.ln1.weight [1152]\n",
      "v.blk.14.ln1.bias [1152]\n",
      "v.blk.14.ffn_down.weight [1152 4304]\n",
      "v.blk.14.ffn_down.bias [4304]\n",
      "v.blk.14.ffn_up.weight [4304 1152]\n",
      "v.blk.14.ffn_up.bias [1152]\n",
      "v.blk.14.ln2.weight [1152]\n",
      "v.blk.14.ln2.bias [1152]\n",
      "v.blk.15.attn_k.weight [1152 1152]\n",
      "v.blk.15.attn_k.bias [1152]\n",
      "v.blk.15.attn_v.weight [1152 1152]\n",
      "v.blk.15.attn_v.bias [1152]\n",
      "v.blk.15.attn_q.weight [1152 1152]\n",
      "v.blk.15.attn_q.bias [1152]\n",
      "v.blk.15.attn_out.weight [1152 1152]\n",
      "v.blk.15.attn_out.bias [1152]\n",
      "v.blk.15.ln1.weight [1152]\n",
      "v.blk.15.ln1.bias [1152]\n",
      "v.blk.15.ffn_down.weight [1152 4304]\n",
      "v.blk.15.ffn_down.bias [4304]\n",
      "v.blk.15.ffn_up.weight [4304 1152]\n",
      "v.blk.15.ffn_up.bias [1152]\n",
      "v.blk.15.ln2.weight [1152]\n",
      "v.blk.15.ln2.bias [1152]\n",
      "v.blk.16.attn_k.weight [1152 1152]\n",
      "v.blk.16.attn_k.bias [1152]\n",
      "v.blk.16.attn_v.weight [1152 1152]\n",
      "v.blk.16.attn_v.bias [1152]\n",
      "v.blk.16.attn_q.weight [1152 1152]\n",
      "v.blk.16.attn_q.bias [1152]\n",
      "v.blk.16.attn_out.weight [1152 1152]\n",
      "v.blk.16.attn_out.bias [1152]\n",
      "v.blk.16.ln1.weight [1152]\n",
      "v.blk.16.ln1.bias [1152]\n",
      "v.blk.16.ffn_down.weight [1152 4304]\n",
      "v.blk.16.ffn_down.bias [4304]\n",
      "v.blk.16.ffn_up.weight [4304 1152]\n",
      "v.blk.16.ffn_up.bias [1152]\n",
      "v.blk.16.ln2.weight [1152]\n",
      "v.blk.16.ln2.bias [1152]\n",
      "v.blk.17.attn_k.weight [1152 1152]\n",
      "v.blk.17.attn_k.bias [1152]\n",
      "v.blk.17.attn_v.weight [1152 1152]\n",
      "v.blk.17.attn_v.bias [1152]\n",
      "v.blk.17.attn_q.weight [1152 1152]\n",
      "v.blk.17.attn_q.bias [1152]\n",
      "v.blk.17.attn_out.weight [1152 1152]\n",
      "v.blk.17.attn_out.bias [1152]\n",
      "v.blk.17.ln1.weight [1152]\n",
      "v.blk.17.ln1.bias [1152]\n",
      "v.blk.17.ffn_down.weight [1152 4304]\n",
      "v.blk.17.ffn_down.bias [4304]\n",
      "v.blk.17.ffn_up.weight [4304 1152]\n",
      "v.blk.17.ffn_up.bias [1152]\n",
      "v.blk.17.ln2.weight [1152]\n",
      "v.blk.17.ln2.bias [1152]\n",
      "v.blk.18.attn_k.weight [1152 1152]\n",
      "v.blk.18.attn_k.bias [1152]\n",
      "v.blk.18.attn_v.weight [1152 1152]\n",
      "v.blk.18.attn_v.bias [1152]\n",
      "v.blk.18.attn_q.weight [1152 1152]\n",
      "v.blk.18.attn_q.bias [1152]\n",
      "v.blk.18.attn_out.weight [1152 1152]\n",
      "v.blk.18.attn_out.bias [1152]\n",
      "v.blk.18.ln1.weight [1152]\n",
      "v.blk.18.ln1.bias [1152]\n",
      "v.blk.18.ffn_down.weight [1152 4304]\n",
      "v.blk.18.ffn_down.bias [4304]\n",
      "v.blk.18.ffn_up.weight [4304 1152]\n",
      "v.blk.18.ffn_up.bias [1152]\n",
      "v.blk.18.ln2.weight [1152]\n",
      "v.blk.18.ln2.bias [1152]\n",
      "v.blk.19.attn_k.weight [1152 1152]\n",
      "v.blk.19.attn_k.bias [1152]\n",
      "v.blk.19.attn_v.weight [1152 1152]\n",
      "v.blk.19.attn_v.bias [1152]\n",
      "v.blk.19.attn_q.weight [1152 1152]\n",
      "v.blk.19.attn_q.bias [1152]\n",
      "v.blk.19.attn_out.weight [1152 1152]\n",
      "v.blk.19.attn_out.bias [1152]\n",
      "v.blk.19.ln1.weight [1152]\n",
      "v.blk.19.ln1.bias [1152]\n",
      "v.blk.19.ffn_down.weight [1152 4304]\n",
      "v.blk.19.ffn_down.bias [4304]\n",
      "v.blk.19.ffn_up.weight [4304 1152]\n",
      "v.blk.19.ffn_up.bias [1152]\n",
      "v.blk.19.ln2.weight [1152]\n",
      "v.blk.19.ln2.bias [1152]\n",
      "v.blk.20.attn_k.weight [1152 1152]\n",
      "v.blk.20.attn_k.bias [1152]\n",
      "v.blk.20.attn_v.weight [1152 1152]\n",
      "v.blk.20.attn_v.bias [1152]\n",
      "v.blk.20.attn_q.weight [1152 1152]\n",
      "v.blk.20.attn_q.bias [1152]\n",
      "v.blk.20.attn_out.weight [1152 1152]\n",
      "v.blk.20.attn_out.bias [1152]\n",
      "v.blk.20.ln1.weight [1152]\n",
      "v.blk.20.ln1.bias [1152]\n",
      "v.blk.20.ffn_down.weight [1152 4304]\n",
      "v.blk.20.ffn_down.bias [4304]\n",
      "v.blk.20.ffn_up.weight [4304 1152]\n",
      "v.blk.20.ffn_up.bias [1152]\n",
      "v.blk.20.ln2.weight [1152]\n",
      "v.blk.20.ln2.bias [1152]\n",
      "v.blk.21.attn_k.weight [1152 1152]\n",
      "v.blk.21.attn_k.bias [1152]\n",
      "v.blk.21.attn_v.weight [1152 1152]\n",
      "v.blk.21.attn_v.bias [1152]\n",
      "v.blk.21.attn_q.weight [1152 1152]\n",
      "v.blk.21.attn_q.bias [1152]\n",
      "v.blk.21.attn_out.weight [1152 1152]\n",
      "v.blk.21.attn_out.bias [1152]\n",
      "v.blk.21.ln1.weight [1152]\n",
      "v.blk.21.ln1.bias [1152]\n",
      "v.blk.21.ffn_down.weight [1152 4304]\n",
      "v.blk.21.ffn_down.bias [4304]\n",
      "v.blk.21.ffn_up.weight [4304 1152]\n",
      "v.blk.21.ffn_up.bias [1152]\n",
      "v.blk.21.ln2.weight [1152]\n",
      "v.blk.21.ln2.bias [1152]\n",
      "v.blk.22.attn_k.weight [1152 1152]\n",
      "v.blk.22.attn_k.bias [1152]\n",
      "v.blk.22.attn_v.weight [1152 1152]\n",
      "v.blk.22.attn_v.bias [1152]\n",
      "v.blk.22.attn_q.weight [1152 1152]\n",
      "v.blk.22.attn_q.bias [1152]\n",
      "v.blk.22.attn_out.weight [1152 1152]\n",
      "v.blk.22.attn_out.bias [1152]\n",
      "v.blk.22.ln1.weight [1152]\n",
      "v.blk.22.ln1.bias [1152]\n",
      "v.blk.22.ffn_down.weight [1152 4304]\n",
      "v.blk.22.ffn_down.bias [4304]\n",
      "v.blk.22.ffn_up.weight [4304 1152]\n",
      "v.blk.22.ffn_up.bias [1152]\n",
      "v.blk.22.ln2.weight [1152]\n",
      "v.blk.22.ln2.bias [1152]\n",
      "v.blk.23.attn_k.weight [1152 1152]\n",
      "v.blk.23.attn_k.bias [1152]\n",
      "v.blk.23.attn_v.weight [1152 1152]\n",
      "v.blk.23.attn_v.bias [1152]\n",
      "v.blk.23.attn_q.weight [1152 1152]\n",
      "v.blk.23.attn_q.bias [1152]\n",
      "v.blk.23.attn_out.weight [1152 1152]\n",
      "v.blk.23.attn_out.bias [1152]\n",
      "v.blk.23.ln1.weight [1152]\n",
      "v.blk.23.ln1.bias [1152]\n",
      "v.blk.23.ffn_down.weight [1152 4304]\n",
      "v.blk.23.ffn_down.bias [4304]\n",
      "v.blk.23.ffn_up.weight [4304 1152]\n",
      "v.blk.23.ffn_up.bias [1152]\n",
      "v.blk.23.ln2.weight [1152]\n",
      "v.blk.23.ln2.bias [1152]\n",
      "v.blk.24.attn_k.weight [1152 1152]\n",
      "v.blk.24.attn_k.bias [1152]\n",
      "v.blk.24.attn_v.weight [1152 1152]\n",
      "v.blk.24.attn_v.bias [1152]\n",
      "v.blk.24.attn_q.weight [1152 1152]\n",
      "v.blk.24.attn_q.bias [1152]\n",
      "v.blk.24.attn_out.weight [1152 1152]\n",
      "v.blk.24.attn_out.bias [1152]\n",
      "v.blk.24.ln1.weight [1152]\n",
      "v.blk.24.ln1.bias [1152]\n",
      "v.blk.24.ffn_down.weight [1152 4304]\n",
      "v.blk.24.ffn_down.bias [4304]\n",
      "v.blk.24.ffn_up.weight [4304 1152]\n",
      "v.blk.24.ffn_up.bias [1152]\n",
      "v.blk.24.ln2.weight [1152]\n",
      "v.blk.24.ln2.bias [1152]\n",
      "v.blk.25.attn_k.weight [1152 1152]\n",
      "v.blk.25.attn_k.bias [1152]\n",
      "v.blk.25.attn_v.weight [1152 1152]\n",
      "v.blk.25.attn_v.bias [1152]\n",
      "v.blk.25.attn_q.weight [1152 1152]\n",
      "v.blk.25.attn_q.bias [1152]\n",
      "v.blk.25.attn_out.weight [1152 1152]\n",
      "v.blk.25.attn_out.bias [1152]\n",
      "v.blk.25.ln1.weight [1152]\n",
      "v.blk.25.ln1.bias [1152]\n",
      "v.blk.25.ffn_down.weight [1152 4304]\n",
      "v.blk.25.ffn_down.bias [4304]\n",
      "v.blk.25.ffn_up.weight [4304 1152]\n",
      "v.blk.25.ffn_up.bias [1152]\n",
      "v.blk.25.ln2.weight [1152]\n",
      "v.blk.25.ln2.bias [1152]\n",
      "v.blk.26.attn_k.weight [1152 1152]\n",
      "v.blk.26.attn_k.bias [1152]\n",
      "v.blk.26.attn_v.weight [1152 1152]\n",
      "v.blk.26.attn_v.bias [1152]\n",
      "v.blk.26.attn_q.weight [1152 1152]\n",
      "v.blk.26.attn_q.bias [1152]\n",
      "v.blk.26.attn_out.weight [1152 1152]\n",
      "v.blk.26.attn_out.bias [1152]\n",
      "v.blk.26.ln1.weight [1152]\n",
      "v.blk.26.ln1.bias [1152]\n",
      "v.blk.26.ffn_down.weight [1152 4304]\n",
      "v.blk.26.ffn_down.bias [4304]\n",
      "v.blk.26.ffn_up.weight [4304 1152]\n",
      "v.blk.26.ffn_up.bias [1152]\n",
      "v.blk.26.ln2.weight [1152]\n",
      "v.blk.26.ln2.bias [1152]\n",
      "v.post_ln.weight [1152]\n",
      "v.post_ln.bias [1152]\n"
     ]
    }
   ],
   "source": [
    "import gguf\n",
    "from gguf import gguf_reader\n",
    "\n",
    "# Load the GGUF file\n",
    "model_path = \"/media/m3/models/MiniCPM-V-2_6/mmproj-model-f16.gguf\"\n",
    "model = gguf_reader.GGUFReader(model_path)\n",
    "\n",
    "for i in range(1000):\n",
    "    try:\n",
    "        print(model.get_tensor(i).name, model.get_tensor(i).shape)\n",
    "    except Exception as e:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownSampleBlock(nn.Module):\n",
    "    def forward(self, x):\n",
    "        vit_embeds = x\n",
    "        h = w = int(vit_embeds.shape[1] ** 0.5)\n",
    "        vit_embeds = vit_embeds.reshape(vit_embeds.shape[0], h, w, -1)\n",
    "        vit_embeds = self.flat_square(vit_embeds)\n",
    "        vit_embeds = vit_embeds.reshape(vit_embeds.shape[0], -1, vit_embeds.shape[-1])\n",
    "        return vit_embeds\n",
    "\n",
    "    def flat_square(self, x):\n",
    "        n, w, h, c = x.size()\n",
    "        if w % 2 == 1:\n",
    "            x = torch.concat([x, torch.zeros((n, 1, h, c), dtype=x.dtype).to(x.device)], dim=1).contiguous()\n",
    "            n, w, h, c = x.size()\n",
    "        if h % 2 == 1:\n",
    "            x = torch.concat([x, torch.zeros((n, w, 1, c), dtype=x.dtype).to(x.device)], dim=2).contiguous()\n",
    "            n, w, h, c = x.size()\n",
    "        x = x.contiguous()\n",
    "        # [batch, w, h/2, c*2]\n",
    "        x = x.view(n, w, int(h / 2), int(c * 2))\n",
    "        # [batch, h/2, w, c*2]\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        # [batch, h/2, w/2, c*4]\n",
    "        x = x.view(n, int(h / 2), int(w / 2), int(c * 4))\n",
    "        # [batch, w/2, h/2, c*4]\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
